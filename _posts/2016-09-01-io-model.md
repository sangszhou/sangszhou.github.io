---
layout: post
title:  "IO 模型和 Java IO"
date:   "2016-09-01 00:00:00"
categories: java
keywords: java, io
---

## Reactor Pattern

有两篇文章谈到 Reactor Pattern, [A1](http://kasunpanorama.blogspot.com/2015/04/understanding-reactor-pattern-with-java.html) and A2(http://www.puncsky.com/blog/2015/01/13/understanding-reactor-pattern-for-highly-scalable-i-o-bound-web-server/)。这让我想起来以前看到过的 C10K 问题 (http://www.kegel.com/c10k.html)。这里对 Reactor 和网络编程的知识进行总结。

Reactor Pattern 经常会会被说成是 Event-driven reactor pattern, 因为它是一个事件驱动的网络编程模型。它的工作方式使用 IO 多路复用，等待消息的到来，消息到来后发到 handler 中处理。它的目的是解决数千个 client 同时连接的问题。而 C10K ，也就是 1 万个 client 同时连接对系统的挑战是非常大的。传统的方式，同步以及 thread-connection 的一一对应问题会限制机器 scale。

此外，协程的概念也被引入。如果有一种更轻量级的进程/线程作为处理单元，而且它们的调度可以做到很快（最好不需要锁），它们在实现上都是试图用一组少量的线程来实现多个任务，一旦某个任务阻塞，则可能用同一线程继续运行其他任务，避免大量上下文的切换。每个协程所独占的系统资源往往只有栈部分。而且，各个协程之间的切换，往往是用户通过代码来显式指定的（跟各种 callback 类似），不需要内核参与，可以很方便的实现异步。

这个技术本质上也是异步非阻塞技术，它是将事件回调进行了包装，让程序员看不到里面的事件循环。程序员就像写阻塞代码一样简单。比如调用 client->recv() 等待接收数据时，就像阻塞代码一样写。实际上是底层库在执行recv时悄悄保存了一个状态，比如代码行数，局部变量的值。然后就跳回到EventLoop中了。什么时候真的数据到来时，它再把刚才保存的代码行数，局部变量值取出来，又开始继续执行。

协程虽然是用户态调度，实际上还是需要调度的，既然调度就会存在上下文切换。所以协程虽然比操作系统进程性能要好，但总还是有额外消耗的。而异步回调是没有切换开销的，它等同于顺序执行代码。所以异步回调程序的性能是要优于协程模型的。

### Why non-blocking IO

A typical server application, such as a web server, needs to process thousands of request concurrently. Therefore, the modern web server needs to meet the following requirements.

1. Handling of thousands of connections simultaneously (significant number of connections may be in idle state as well)
2. Handling high latency connections
3. Request/response handling needs to be decoupled
4. Minimize latency, maximize throughput and avoiding unnecessary CPU cycles

Hence, to cater to such requirements, there can be several possibilities in the server application architecture.

1. Having a pool of sockets for each client, and periodically polling them: This is the most straightforward approach; however, it is more or less impractical without non-blocking sockets. This is extremely inefficient and never scales with the increasing number of connections.
2. Thread per socket: This is the conventional approach and was initially used in some applications and this is the only practical solution with blocking sockets. Having thread per each client connection has several drawbacks, such as a large amount of overhead with the thread scheduling. Another flaw in this approach is that as the number of connections/client increases, we need to increase the number of threads as well. Therefore, this too hardly scales.
3. Readiness selection: It is the ability to choose a socket that will not block when read or written. This is a very efficient way of handling thousands of concurrent clients and scales well.

The readiness selection approach was originally presented as an object behavioral pattern by Schmidt [1] under ‘Reactor: An Object Behavioral Pattern for Demultiplexing and Dispatching Handles for Synchronous Events’ paper. Therefore, in order to understand the readiness selection better, we need to take a closer look at the Reactor pattern.

**Reactor pattern**

The reactor design pattern was introduced as a general architecture to implement event-driven systems. In order to solve our original problem of implementing a server application that can handle thousands of simultaneous client connections, Reactor pattern provides a way in which we can listen to the events (incoming connections/requests) with a synchronous demultiplexing strategy, so that when an incoming event occurs, it is dispatched to a service provider (handler) that can handle this event.

Let's have a detailed look at each key participants in Reactor pattern, which are depicted in the following class diagram.

![reactor_pattern](/images/posts/network/reactor_pattern_1.png)

In the reactor pattern, the initiation dispatcher is the most crucial component. Often this is also known as the ‘Reactor’. For each type of service that the server application offers, it introduces a separate event handler that can process that particular event type. All these event handlers are registered with the initiation dispatcher. Initiation dispatcher uses a demultiplexer that can listen to all the incoming events and notify the initiation dispatcher accordingly. The demultiplexer uses ‘handles’ to identify the events that occur on a given resource, such as network connection. Handles are often used to identify OS managed resources, such as network connections, open files, etc.


The behavior of demultiplexer is synchronous such that it blocks on waiting for events to occur on a set of handlers. However, once the event occurs, it simply notifies the initiation dispatcher and it will hand over the event to the respective concrete event handler type.

**Reactor in Java**

When it comes to developing server applications with Java, we need to have an underlying framework that supports a realization of the Reactor pattern. With the Java NIO framework, JDK provides the necessary building blocks to implement Reactor pattern with Java.

![pic](/images/posts/network/reactor_java_framework.png)

```java
	ServerSocketChannel server = ServerSocketChannel.open();
	server.socket().bind(new InetSocketAddress(port));
	server.configureBlocking(false);

	Reactor reactor = new Reactor();
	reactor.registerChannel(SelectionKey.OP_ACCEPT, server);

	reactor.registerEventHandler(SelectionKey.OP_ACCEPT, new AcceptEventHandler(reactor.getDemultiplexer()));


	reactor.registerEventHandler(SelectionKey.OP_READ, new ReadEventHandler(reactor.getDemultiplexer()));
	reactor.registerEventHandler(SelectionKey.OP_WRITE, new WriteEventHandler());
	reactor.run();
```

```java
public class Reactor {
    private Map<Integer, EventHandler> registeredHandlers = new ConcurrentHashMap<Integer, EventHandler>();
    private Selector demultiplexer;

    public Reactor() throws Exception {
        demultiplexer = Selector.open();
    }

    public Selector getDemultiplexer() {
        return demultiplexer;
    }

    public void registerEventHandler( int eventType, EventHandler eventHandler) {
        registeredHandlers.put(eventType, eventHandler);
    }

    public void registerChannel(int eventType, SelectableChannel channel) throws Exception {
        channel.register(demultiplexer, eventType);
    }

    public void run() {
        try {
            while (true) { // Loop indefinitely
                demultiplexer.select();

                Set<SelectionKey> readyHandles = demultiplexer.selectedKeys();
                Iterator<SelectionKey> handleIterator = readyHandles.iterator();

                while (handleIterator.hasNext()) {
                    SelectionKey handle = handleIterator.next();

                    if (handle.isAcceptable()) {
                        EventHandler handler = registeredHandlers.get(SelectionKey.OP_ACCEPT);
                        handler.handleEvent(handle);
                    }

                    if (handle.isReadable()) {
                        EventHandler handler = registeredHandlers.get(SelectionKey.OP_READ);
                        handler.handleEvent(handle);
                        handleIterator.remove();
                    }

                    if (handle.isWritable()) {
                        EventHandler handler = registeredHandlers.get(SelectionKey.OP_WRITE);
                        handler.handleEvent(handle);
                        handleIterator.remove();
                    }
                }
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
```

上面这个例子的更多代码可以参考 [github]()

**常规的，Connection 与 Thread 一一对应方式**

```java
class ReactiveEchoServer implements Runnable {
    private final Selector _selector;
    private final ServerSocketChannel _serverSocketChannel;
    private static final int WORKER_POOL_SIZE = 10;
    private static ExecutorService _workerPool;

    ReactiveEchoServer(int port) throws IOException {
        _selector = Selector.open();
        _serverSocketChannel = ServerSocketChannel.open();
        _serverSocketChannel.socket().bind(new InetSocketAddress(port));
        _serverSocketChannel.configureBlocking(false);

        // Register _serverSocketChannel with _selector listening on OP_ACCEPT events.
       // Callback: Acceptor, selected when a new connection incomes.
        SelectionKey selectionKey = _serverSocketChannel.register(_selector, SelectionKey.OP_ACCEPT);
        selectionKey.attach(new Acceptor());
    }

    public void run() {
        try {
            // Event Loop
            while (true) {
                _selector.select();
                Iterator it = _selector.selectedKeys().iterator();

                while (it.hasNext()) {
                    SelectionKey sk = (SelectionKey) it.next();
                    it.remove();
                    Runnable r = (Runnable) sk.attachment(); // handler or acceptor callback/runnable
                    if (r != null) {
                        r.run();
                    }
                }
            }
        }
        catch (IOException ex) {
            ex.printStackTrace();
        }
    }

    public static ExecutorService getWorkerPool() {
        return _workerPool;
    }

    // Acceptor: if connection is established, assign a handler to it.
    private class Acceptor implements Runnable {
        public void run() {
            try {
                SocketChannel socketChannel = _serverSocketChannel.accept();
                if (socketChannel != null) {
                    new Handler(_selector, socketChannel);
                }
            }
            catch (IOException ex) {
                ex.printStackTrace();
            }
        }
    }

    public static void main(String[] args) {
        _workerPool = Executors.newFixedThreadPool(WORKER_POOL_SIZE);

        try {
            new Thread(new ReactiveEchoServer(9090)).start(); // a single thread blocking on selector for events
        } catch (IOException ex) {
            ex.printStackTrace();
        }
    }
}
```

```java
class Handler implements Runnable {
   private final SocketChannel _socketChannel;
   private final SelectionKey _selectionKey;

   private static final int READ_BUF_SIZE = 1024;
   private static final int WRiTE_BUF_SIZE = 1024;
   private ByteBuffer _readBuf = ByteBuffer.allocate(READ_BUF_SIZE);
   private ByteBuffer _writeBuf = ByteBuffer.allocate(WRiTE_BUF_SIZE);

   public Handler(Selector selector, SocketChannel socketChannel) throws IOException {
       _socketChannel = socketChannel;
       _socketChannel.configureBlocking(false);

       // Register _socketChannel with _selector listening on OP_READ events.
       // Callback: Handler, selected when the connection is established and ready for READ
       _selectionKey = _socketChannel.register(selector, SelectionKey.OP_READ);
       _selectionKey.attach(this);
       selector.wakeup(); // let blocking select() return
   }

   public void run() {
       try {
           if (_selectionKey.isReadable()) {
               read();
           }
           else if (_selectionKey.isWritable()) {
               write();
           }
       }
       catch (IOException ex) {
           ex.printStackTrace();
       }
   }

   // Process data by echoing input to output
   synchronized void process() {
       _readBuf.flip();
       byte[] bytes = new byte[_readBuf.remaining()];
       _readBuf.get(bytes, 0, bytes.length);
       System.out.print("process(): " + new String(bytes, Charset.forName("ISO-8859-1")));

       _writeBuf = ByteBuffer.wrap(bytes);

       // Set the key's interest to WRITE operation
       _selectionKey.interestOps(SelectionKey.OP_WRITE);
       _selectionKey.selector().wakeup();
   }

   synchronized void read() throws IOException {
       try {
           int numBytes = _socketChannel.read(_readBuf);
           System.out.println("read(): #bytes read into '_readBuf' buffer = " + numBytes);

           if (numBytes == -1) {
               _selectionKey.cancel();
               _socketChannel.close();
               System.out.println("read(): client connection might have been dropped!");
           }
           else {
               ReactiveEchoServer.getWorkerPool().execute(new Runnable() {
                   public void run() {
                       process();
                   }
               });
           }
       }
       catch (IOException ex) {
           ex.printStackTrace();
           return;
       }
   }

   void write() throws IOException {
       int numBytes = 0;

       try {
           numBytes = _socketChannel.write(_writeBuf);
           System.out.println("write(): #bytes read from '_writeBuf' buffer = " + numBytes);

           if (numBytes > 0) {
               _readBuf.clear();
               _writeBuf.clear();

               // Set the key's interest-set back to READ operation
               _selectionKey.interestOps(SelectionKey.OP_READ);
               _selectionKey.selector().wakeup();
           }
       }
       catch (IOException ex) {
           ex.printStackTrace();
       }
   }
}
```

## 概念

阻塞和非阻塞:

> 就函数的实现而言, 数据在就绪之前是等待还是立刻返回

同步和异步:

> 一个 IO 是指文件从磁盘到内核缓冲区再到用户空间的过程。不同和异步的区别就在于数据
> 从内核缓冲区到用户空间这个过程是否需要用户程序等待

这里是从操作系统角度对同步异步的理解, 如果从 API 的角度来讲, 一个 API 是异步的说的
是这个 API 是否会等待数据的返回。按照 akka doc 的说法就是 "a method call is considered
as synchronous if the caller cannot make progress until the method returns with
a value or throws an exception"

## IO 模型

### 同步阻塞

网络编程中, 服务器读取客户端的数据要调用 recvfrom。在默认情况下, 这个调用会一直阻塞
到数据接受完毕, 这就是一个同步阻塞的 IO

![](/images/posts/linux/blockSynchronousIO.png)

### 同步非阻塞

网络编程中, 把 SOCKET 设置为 NONBLOCK 即可, 此方式仅对网络编程有效, 对磁盘文件无效。

![](/images/posts/linux/SynchronousUnblockingIO.png)

### IO 多路复用

如果有很多网络传输, 创建多个 socket 每个 socket 负责一个连接有些浪费线程资源, 一个更好的
办法是由一个中控系统负责 handle 所有的连接和数据传输请求, 当有请求发生时, 通过后端, 这样
可以实现较少的线程和较高的吞吐量 (表达的不好)

![](/images/posts/linux/IOMultiplexing.png)

在 linux 系统中, 实现 IO 多路复用的函数有三个, 分别是 select, poll 和 epoll.
其中 select 和 poll 的原理基本相同:

1. 注册监听的 fd (注册时最好用非阻塞的)
2. 每次调用都会检查这些 fd 的状态

相比于 select, poll 解决了单个进程可以打开文件描述符有数量限制这个问题。 poll 通过
一个 pollfd 数组向内核传递需要关注的事件, 避开了文件描述符的数量限制。

select 和 poll 的问题在于包含 fd 的数组被整体复制与内核和用户空间之间, 开销随着 fd 的
增多而线性增大, 而基于事件驱动的 epoll 解决了这个问题:

1. 基于事件驱动, 避免了每次都要把所有的 fd 都扫一遍
2. epoll_wait 只返回就绪的 fd
3. epoll 使用 nmap 内存映射技术避免了内存复制的开销
4. epoll 的 fd 数量是操作系统的最大文件句柄数目, 这个数目一般很大 (ulimit -a)

此外，对于IO复用还有一个水平触发和边缘触发的概念：

* 水平触发：当就绪的fd未被用户进程处理后，下一次查询依旧会返回，这是select和poll的触发方式。
* 边缘触发：无论就绪的fd是否被处理，下一次不再返回。理论上性能更高，但是实现相当复杂，并且任何意外的丢失事件都会造成请求处理错误。epoll默认使用水平触发，通过相应选项可以使用边缘触发。


### 信号驱动

![](/images/posts/linux/SignalDrivenIO.png)

* 开启套接字信号驱动 IO 功能
* 用户程序调用 sigaction 执行信号处理函数
* 数据就绪, 生成 sigio 信号, 通过信号回调通知应用程序读取数据

### 异步非阻塞

![](/images/posts/linux/AsynchronousUnblockingIO.png)

数据已经拷贝到用户空间后再通知程序

## Java 网络编程

### BIO
典型的网络编程模型, 通常是我们实现一个服务端程序, 步骤如下:

* 主线程 accept 请求阻塞
* 请求到达, 创建新线程处理这个套接字, 完成对客户端的响应
* 主线程继续 accept 下一个请求

### NIO

NIO 的实现流程有点像 select:

* 创建ServerSocketChannel监听客户端连接并绑定监听端口，设置为非阻塞模式
* 创建Reactor线程，创建多路复用器(Selector)并启动线程
* 将ServerSocketChannel注册到Reactor线程的Selector上。监听accept事件
* Selector在线程run方法中无线循环轮询准备就绪的Key
* Selector监听到新的客户端接入，处理新的请求，完成tcp三次握手，建立物理连接
* 将新的客户端连接注册到Selector上，监听读操作。读取客户端发送的网络消息
* 客户端发送的数据就绪则读取客户端请求，进行处理

### AIO
Java7 引入 NIO2.0, 提供了异步文件通道和异步套接字通道的实现, 相当于 Linux 中的异步 IO

* 创建AsynchronousServerSocketChannel，绑定监听端口
* 调用AsynchronousServerSocketChannel的accpet方法，传入自己实现的CompletionHandler。包括上一步，都是非阻塞的
* 连接传入，回调CompletionHandler的completed方法，在里面，调用AsynchronousSocketChannel的read方法，传入负责处理数据的CompletionHandler。
* 数据就绪，触发负责处理数据的CompletionHandler的completed方法。继续做下一步处理即可。
* 写入操作类似，也需要传入CompletionHandler。

```java
public class Server {
	private static Charset charset = Charset.forName("US-ASCII");
    private static CharsetEncoder encoder = charset.newEncoder();

	public static void main(String[] args) throws Exception {
		AsynchronousChannelGroup group = AsynchronousChannelGroup.withThreadPool(Executors.newFixedThreadPool(4));
		AsynchronousServerSocketChannel server = AsynchronousServerSocketChannel.open(group).bind(new InetSocketAddress("0.0.0.0", 8013));
		server.accept(null, new CompletionHandler<AsynchronousSocketChannel, Void>() {
			@Override
			public void completed(AsynchronousSocketChannel result, Void attachment) {
				server.accept(null, this); // 接受下一个连接
				try {
					 String now = new Date().toString();
					 ByteBuffer buffer = encoder.encode(CharBuffer.wrap(now + "\r\n"));
					//result.write(buffer, null, new CompletionHandler<Integer,Void>(){...}); //callback or
					Future<Integer> f = result.write(buffer);
					f.get();
					System.out.println("sent to client: " + now);
					result.close();
				} catch (IOException | InterruptedException | ExecutionException e) {
					e.printStackTrace();
				}
			}
			@Override
			public void failed(Throwable exc, Void attachment) {
				exc.printStackTrace();
			}
		});
		group.awaitTermination(Long.MAX_VALUE, TimeUnit.SECONDS);
	}
}
```

### Reactor

**单线程模型**

这是最简单的单Reactor单线程模型。Reactor线程是个多面手，负责多路分离套接字，Accept新连接，并分派请求到处理器链中。 该模型
适用于处理器链中业务处理组件能快速完成的场景。不过，这种单线程模型不能充分利用多核资源，所以实际使用的不多

![](/images/posts/java/reactor_model_1.png)

**多线程模型**

该模型在处理器链部分采用了多线程（线程池），也是后端程序常用的模型

![](/images/posts/java/reactor_model_2.png)

**第三种模型**比起第二种模型，是将Reactor分成两部分，mainReactor负责监听server socket，accept新连接，并
将建立的socket分派给subReactor。subReactor负责多路分离已连接的socket，读写网络数据，对业务处理功能，其扔给worker线程池完成。
通常，subReactor个数上可与CPU个数等同

kafka 就是第三种线程模型

```scala
Acceptor:
def run() {
    if(key.isAcceptable)
        accept(key, processors(currentProcessor))
}

def accept(key: SelectionKey, processor: Processor) {
    processor.accept(socketChannel)
}

Processor:
def accept(socketChannel: SocketChannel) {
    newConnections.add(socketChannel)
    wakeup()        
}
override def run() {
    while(isRunning) {
        configureNewConnections()
        processNewResponses()
        val ready = selector.select(300) // wakeup 能让 select 理解返回
    }
}
```

ConnectionList 是 Acceptor 和 Processor 交互的接口

![](/images/posts/java/reactor_model_3.png)

说完 Reactor 模型的三种形式，那么Netty是哪种呢？其实，我还有一种Reactor模型的变种没说，那就是去掉线程池的第三种形式的变种，
这也 是Netty NIO的默认模式。在实现上，Netty中的Boss类充当mainReactor，NioWorker类充当subReactor（默认 NioWorker的个数是
Runtime.getRuntime().availableProcessors()）。在处理新来的请求 时，NioWorker读完已收到的数据到ChannelBuffer中，之
后触发ChannelPipeline中的ChannelHandler流。

Netty是事件驱动的，可以通过ChannelHandler链来控制执行流向。因为ChannelHandler链的执行过程是在 subReactor中同步的，
所以如果业务处理handler耗时长，将严重影响可支持的并发数。这种模型适合于像Memcache这样的应用场景，但对需要操作数据库或者和其他模块阻塞交互的系统就不是很合适。

Netty的可扩展性非常好，而像ChannelHandler线程池化的需要，可以通过在 ChannelPipeline 中添加 Netty 内置的 ChannelHandler 实现类 ExecutionHandler 实现，
对使用者来说只是 添加一行代码而已。对于ExecutionHandler需要的线程池模型，Netty提供了两种可 选：

1） MemoryAwareThreadPoolExecutor 可控制Executor中待处理任务的上限（超过上限时，后续进来的任务将被阻 塞），并可控制单个Channel待处理任务的上限；

2） OrderedMemoryAwareThreadPoolExecutor 是  MemoryAwareThreadPoolExecutor 的子类，它还可以保证同一Channel中处理的事件流的顺序性，这主要是控制事件在异步处
理模式下可能出现的错误的事件顺序，但它并不保证同一Channel中的事件都在一个线程中执行（通常也没必要）。
一般来说，OrderedMemoryAwareThreadPoolExecutor 是个很不错的选择，当然，如果有需要，也可以DIY一个。


### Netty 的线程模型

**server 端源码分析**

```java
    public static void main(String args[]) throws Exception {

        EventLoopGroup bossGroup = new NioEventLoopGroup(1);
        EventLoopGroup workerGroup = new NioEventLoopGroup();

        try {
            ServerBootstrap bootstrap = new ServerBootstrap();
            bootstrap.group(bossGroup, workerGroup)
                    .channel(NioServerSocketChannel.class)
                    .option(ChannelOption.SO_BACKLOG, 100)
                    .handler(new LoggingHandler(LogLevel.INFO))
                    .childHandler(new ChannelInitializer<SocketChannel>() {
                        @Override
                        protected void initChannel(SocketChannel ch) throws Exception {
                            ChannelPipeline channelPipeline = ch.pipeline();
                            channelPipeline.addLast(new EchoServerHandler());

                        }
                    });

            //start server
            ChannelFuture f = bootstrap.bind(PORT).sync();

            f.channel().closeFuture().sync();

        } finally {
            bossGroup.shutdownGracefully();
            workerGroup.shutdownGracefully();
        }
```

bossGroup线程组实际就是Acceptor线程池，负责处理客户端的TCP连接请求，如果系统只有一个服务端端口需要监听，则建议bossGroup线程组线程数设置为1。

workerGroup是真正负责I/O读写操作的线程组，通过 ServerBootstrap 的 group 方法进行设置，用于后续的Channel绑定。

![](/images/posts/network/netty_process.jpg)

Netty采用了串行化设计理念，从消息的读取、编码以及后续 Handler 的执行，始终都由 IO 线程 NioEventLoop 负责，
这就意外着整个流程不会进行线程上下文的切换，数据也不会面临被并发修改的风险，对于用户而言，甚至不需要了解Netty的线程细节，这确实是个非常好的设计理念，它的工作原理图如下：

![](/images/posts/network/netty_pipeline.jpg)


**上面写 client 的解决方法是, 每个 Connection 有一个专属的 NioEventLoopGroup, Bootstrap**

### Netty, Mina // @todo

### 参考资料
[也谈IO模型](http://www.rowkey.me/blog/2016/01/18/io-model/#%E5%89%8D%E8%A8%80)
