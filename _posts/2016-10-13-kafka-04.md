---
layout: post
title: Kakfa 04
categories: [kafka]
keywords: kafka
---

## 0.9 Consumer Client Re-Design

### Motivation

We have a lot of users who have expressed interest in using and writing non-java clients. Currently, this is pretty straightfoward for the SimpleConsumer but not for the high level consumer. The high level consumer does some complex failure detection and rebalancing, which is non-trivial to re-implement correctly.

The goal is to have a very thin consumer client, with minimum dependencies to make this easy for the users.

**Central co-ordination :**

1. The current version of the high level consumer suffers from herd and split brain problems, 
   where multiple consumers in a group run a distributed algorithm to agree on the same partition 
   ownership decision. Due to different view of the zookeeper data, they run into conflicts that makes 
   the rebalancing attempt fail. But there is no way for a consumer to verify if a rebalancing operation 
   completed successfully on the entire group. This also leads to some potential bugs in the rebalancing 
   logic, for example, https://issues.apache.org/jira/browse/KAFKA-242
2. This can be mediated by moving the failure detection and rebalancing logic to a centralized highly-available co-ordinator - Kafka 0.9 Consumer Rewrite Design

**Allow manual partition assignment**

**Allow manual offset management**

目前的 client 设置, API 的灵活度不高, 如果想要自己来管理 offset, 那么就得使用 simpleConsumer, 就要
放弃 client 的集群管理和自动负载均衡问题, 如果要自动管理集群和自动负载均衡就不能调控 offset, 两者不可
兼得, 这是一个不够灵活的点。此外, 我可能想要一个 API, 最长等待时间是 3s, 3s 后能拿多少数据就返回多少数据。

**Non blocking consumer APIs**

**Invocation of user specified callback on rebalance**

Currently Kafka have two types of consumers: high-level consumer and simple consumer. In simple 
consumer user can specify broker-partition and offset, but there is no failover/re-balance support. 
So users with requirements 3 and 4 but no requirement for group/re-balance would more prefer 
to use the simple consumer. Basically the high-level consumer provides the following main 
functionalities against simple consumer:

1. Auto/Hidden Offset Management
2. Auto(Simple) Partition Assignment
3. Broker Failover => Auto Rebalance
4. Consumer Failover => Auto Rebalance
5. If user do not want any of these, then simple consumer is sufficient
6. If user want to control over offset management with others unchanged, one option is to expose the current ZK implementation of the high-level consumer to users and allow them to override; another option is to change the high-level consumer API to return the offset vector associated with messages
7. If user want to control partition assignment, one option is to change the high-level consumer API to allow such config info be passed in while creating the stream; another option is ad-hoc: just make a single-partition topic and assign it to the consumer.
8. If user just want the automatic partition assignment be more "smart" with co-location consideration, etc, one option is to store the host/rack info in ZK and let the rebalance algorithm read them while doing the computation.

## Efficient data transfer through zero copy

[link written by indian people](https://www.ibm.com/developerworks/library/j-zerocopy/)

Each time data traverses the user-kernel boundary, it must be copied, which consumes CPU cycles and memory bandwidth. Fortunately, you can eliminate these copies through a technique called — appropriately enough — zero copy. Applications that use zero copy request that the kernel copy the data directly from the disk file to the socket, without going through the application. Zero copy greatly improves application performance and reduces the number of context switches between kernel and user mode.

The Java class libraries support zero copy on Linux and UNIX systems through the transferTo() method in 
java.nio.channels.FileChannel. You can use the transferTo() method to transfer bytes directly from the 
channel on which it is invoked to another writable byte channel, without requiring data to flow through 
the application. This article first demonstrates the overhead incurred by simple file transfer done 
through traditional copy semantics, then shows how the zero-copy technique using transferTo() achieves 
better performance.

### Date transfer: The traditional approach

```java
File.read(fileDesc, buf, len);
Socket.send(socket, buf, len);
```

Although Listing 1 is conceptually simple, internally, the copy operation requires four context 
switches between user mode and kernel mode, and the data is copied four times before the operation 
is complete. Figure 1 shows how data is moved internally from the file to the socket:

![](/images/posts/java/disktosocket.gif)

The read() call causes a context switch (see Figure 2) from user mode to kernel mode. 
Internally a sys_read() (or equivalent) is issued to read the data from the file. 
The first copy (see Figure 1) is performed by the direct memory access (DMA) engine, which 
reads file contents from the disk and stores them into a **kernel address space buffer**.

The requested amount of data is copied from the read buffer into the user buffer, and the read() call 
returns. The return from the call causes another context switch from kernel back to user mode. 
Now the data is stored in the **user address space buffer**.

The send() socket call causes a context switch from user mode to kernel mode. A third 
copy is performed to put the data into a kernel address space buffer again. This time, 
though, the data is put into a different buffer, one that is associated with the destination socket.

The send() system call returns, creating the fourth context switch. Independently 
and asynchronously, a fourth copy happens as the DMA engine passes the data from the kernel 
buffer to the protocol engine.

**kernel buffer 存在的意义**

Use of the intermediate kernel buffer (rather than a direct transfer of the data into the user buffer) 
might seem inefficient. But intermediate kernel buffers were introduced into the process to improve 
performance. Using the intermediate buffer on the read side allows the kernel buffer to act as 
a "readahead cache" when the application hasn't asked for as much data as the kernel buffer 
holds. This significantly improves performance when the requested data amount is less than the 
kernel buffer size. The intermediate buffer on the write side allows the write to complete asynchronously.

Unfortunately, this approach itself can become a performance bottleneck if the size of the data 
requested is considerably larger than the kernel buffer size. The data gets copied multiple 
times among the disk, kernel buffer, and user buffer before it is finally delivered to the application.

### Data transfer: The zero-copy approach

If you re-examine the traditional scenario, you'll notice that the second and third data 
copies are not actually required. The application does nothing other than cache the data 
and transfer it back to the socket buffer. Instead, the data could be transferred directly 
from the read buffer to the socket buffer. The transferTo() method lets you do exactly this. 
Listing 2 shows the method signature of transferTo():

```java
public void transferTo(long position, long count, WritableByteChannel target);
```

The transferTo() method transfers data from the file channel to the given writable byte channel. 
Internally, it depends on the underlying operating system's support for zero copy; 
in UNIX and various flavors of Linux, this call is routed to the sendfile() system call, 
shown in Listing 3, which transfers data from one file descriptor to another:

```java
#include <sys/socket.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```

![](/images/posts/java/copydatawithdirectbuffer.gif)

The transferTo() method causes the file contents to be copied into a read buffer by the DMA engine. Then the data is copied by the kernel into the kernel buffer associated with the output socket.

The third copy happens as the DMA engine passes the data from the kernel socket buffers to the protocol engine.

This is an improvement: we've reduced the number of context switches from four to two and reduced the number of data copies from four to three (only one of which involves the CPU). But this does not yet get us to our goal of zero copy. We can further reduce the data duplication done by the kernel if the underlying network interface card supports gather operations. In Linux kernels 2.4 and later, the socket buffer descriptor was modified to accommodate this requirement. This approach not only reduces multiple context switches but also eliminates the duplicated data copies that require CPU involvement. The user-side usage still remains the same, but the intrinsics have changed:

The transferTo() method causes the file contents to be copied into a kernel buffer by the DMA engine.

No data is copied into the socket buffer. Instead, only descriptors with information about the location and length of the data are appended to the socket buffer. The DMA engine passes data directly from the kernel buffer to the protocol engine, thus eliminating the remaining final CPU copy.

还是又一次 copy 的, 就是把 file 文件内容拷贝到 kernel buffer, 然后修改 socket buffer 的开始位置和 length, 
然后就能直接发送数据到 socket 了

```java
	    SocketAddress sad = new InetSocketAddress(host, port);
	    SocketChannel sc = SocketChannel.open();
	    sc.connect(sad);
	    sc.configureBlocking(true);

	    String fname = "sendfile/NetworkInterfaces.c";
	    long fsize = 183678375L, sendzise = 4094;
	    
	    // FileProposerExample.stuffFile(fname, fsize);
	    FileChannel fc = new FileInputStream(fname).getChannel();
            long start = System.currentTimeMillis();
	    long nsent = 0, curnset = 0;
	    curnset =  fc.transferTo(0, fsize, sc);

```

## Idempotent Producer

### Introduction

Kafka provides "at least once" delivery semantics. This means that a message that is sent may delivered 
one or more times. What people really want is "exactly once" semantics whereby duplicate messages 
are not delivered.

There are two common reasons duplicate messages may occur:

1. If a client attempts to send a message to the cluster and gets a network error 
   then retrying will potentially lead to duplicates. If network error occurred before the 
   message was delivered, no duplication will occur. However if the network error occurs after the 
   message is appended to the log but before the response can be delivered to the sender the sender is 
   left not knowing what has happened. The only choices are to retry and risk duplication or to 
   give up and declare the message lost.
2. If a consumer reads a message from a topic and then crashes then when the consumer 
   restarts or another instances takes over consumption the new consumer will start 
   from the last known position of the original consumer.

The second case can be handled by consumers by making use of the **offset** Kafka provides. 
They can store the offset with their output and then ensure that the new consumer always 
picks up from the last stored offset. Or, they can use the offset as a kind of key and use 
it to deduplicate their output in whatever final destination system they populate.

The first case currently has no good solution, however. The client doesn't know the offset 
of the message so it has no unique way to identify the message and check if the send succeeded.

For consumers that correctly handle duplicates, this proposal would strengthen the guarantees provided 
by Kafka to what is often called **"atomic broadcast"**.

This proposal will introduce an optional set of ids that will provide a unique identifier for messages to avoid duplicates.

### Producer implementations that don't care about idempotency should not need to do anything special.

Consider a more elaborate use case which involves copying data from a source to a Kafka topic. 
This would be the case with **Mirror Maker**, for example, or any "stream processing" use case. 
We want it to be the case that the process doing the population can periodically save its position 
in the upstream topic/database and always resume from this saved position. In the event of a 
crash we want the copy process to be able to resume from the last known position without 
producing duplicates in the destination topic. To accomplish this the copy process can save 
BOTH its input offset/position AND the ids we will introduce associated with its downstream topic. 
When it restarts after a crash it will initialize with the saved ids. This will effectively make 
the duplicate produce requests the same as the network error retry case described above.

**Pipelining** 的问题

A related need is the ability to pipeline produce requests safely in the presence of retries. 
When combined with retries this can lead to messages being stored out of order. If the sender 
sends messages M1, M2, M3 asynchronously without waiting for responses it may then receive a 
success for M1 and M3 but an error for M2. If it retries M2 successfully this will lead to the 
topic containing the messages in the order M1, M3, M2.

**Fault tolerance**

A common cause of errors is actual broker failure. If a broker fails with a request outstanding 
and unacknowledged you don't know if the newly elected master contains the message or not and 
will want to retry your request. Thus the idempotency mechanism needs to work even in the presence 
of broker failures.

**Fencing**

Another twist to this is that it in the mirror maker or other cases where consumer failure 
is automatically detected it is possible to have false positives leading to a situation where 
at least transiently we have two consumers reading the same input and producing the same output. 
It is important that we handle this "split brain" problem correctly and gracefully.

**Fencing**

Another twist to this is that it in the mirror maker or other cases where consumer failure 
is automatically detected it is possible to have false positives leading to a situation where at 
least transiently we have two consumers reading the same input and producing the same output. 
It is important that we handle this "split brain" problem correctly and gracefully.

### Proposed Implementation

1. At most once 消息可能会丢，但绝不会重复传输
2. At least one 消息绝不会丢，但可能会重复传输
3. Exactly once 每条消息肯定会被传输一次且仅传输一次，很多时候这是用户所想要的。　　

当Producer向broker发送消息时，一旦这条消息被commit，因数replication的存在，它就不会丢。
但是如果Producer发送数据给broker后，遇到网络问题而造成通信中断，那Producer就无法判断该条消息是否已经commit。
虽然Kafka无法确定网络故障期间发生了什么，但是Producer可以生成一种类似于主键的东西，发生故障时幂等性的重试多次，
这样就做到了Exactly once。目前这一Feature还并未实现，有希望在Kafka未来的版本中实现。（所以目前默认情况下一条消
息从Producer到broker是确保了At least once，可通过设置Producer异步发送实现 At most once)。

接下来讨论的是消息从broker到Consumer的delivery guarantee语义。（仅针对Kafka consumer high level API)。
Consumer在从broker读取消息后，可以选择commit，该操作会在Zookeeper中保存该Consumer在该Partition中读取的消息的offset。
该Consumer下一次再读该Partition时会从下一条开始读取。如未commit，下一次读取的开始位置会跟上一次commit之后的开始位置相同。
当然可以将Consumer设置为autocommit，即Consumer一旦读到数据立即自动commit。如果只讨论这一读取消息的过程，
那Kafka是确保了Exactly once。但实际使用中应用程序并非在Consumer读取完数据就结束了，而是要进行进一步处理，
而数据处理与commit的顺序在很大程度上决定了消息从broker和consumer的消息投递语义保证。

读完消息先commit消费状态(保存offset)再处理消息。这种模式下，如果Consumer在commit后还没来得及处理消息就crash了，
下次重新开始工作后就无法读到刚刚已提交而未处理的消息，这就对应于At most once

读完消息先处理再commit消费状态(保存offset)。这种模式下，如果在处理完消息之后commit之前Consumer crash了，
下次重新开始工作时还会处理刚刚未commit的消息，实际上该消息已经被处理过了。这就对应于At least once。
在很多使用场景下，消息都有一个主键，所以消息的处理往往具有幂等性，即多次处理这一条消息跟只处理一次是等效的，
那就可以认为是Exactly once。（个人感觉这种说法比较牵强，毕竟它不是Kafka本身提供的机制，主键本身也并不
能完全保证操作的幂等性。而且实际上我们说delivery guarantee语义是讨论被处理多少次，而非处理结果怎样，
因为处理方式多种多样，我们不应该把处理过程的特性——如是否幂等性，当成Kafka本身的Feature）

如果一定要做到Exactly once，就需要协调offset和实际操作的输出。经典的做法是引入两阶段提交。如果能让offset和
操作输入存在同一个地方，会更简洁和通用。这种方式可能更好，因为许多输出系统可能不支持两阶段提交。比如，Consumer拿
到数据后可能把数据放到HDFS，如果把最新的offset和数据本身一起写到HDFS，那就可以保证数据的输出和offset的更新
要么都完成，要么都不完成，间接实现Exactly once。（目前就high level API而言，offset是存于Zookeeper中的，
无法存于HDFS，而low level API的offset是由自己去维护的，可以将之存于HDFS中）

总之，Kafka默认保证At least once，并且允许通过设置Producer异步提交来实现At most once。而Exactly once要求与外部存储系统协作，幸运的是Kafka提供的offset可以非常直接非常容易得使用这种方式。

[link](http://blog.csdn.net/lizhitao/article/details/44199257)

## Fail over

broker fail over, zookeeper fail over, controller fail over, add/delete partition, 
add/remove partition

### broker fail over

Broker 的 Failover，可以分为两个过程，一个是 broker failure， 一个是 broker startup。

新加 broker

在谈 failover 之前，我们先看一个更简单的过程，就是新加一个全新的 broker:

首先明确，**新加的 broker 对现存所有的 topic 和 partition，不会有任何影响**

因为一个 topic 的 partition 的所有 replica 的 assignment 情况，在创建时就决定了，并不会自动发生变化,
除非你手动的去做 reassignment 

所以新加一个 broker，所需要做的只是大家同步一下元数据，大家都知道来了一个新的 broker,
当你创建新的 topic 或 partition 的时候，它会被用上

**Broker Failure**

首先明确，这里的 broker failure，并不一定是 broker server 真正的 dead了， 只是指该 broker 
所对应的 zk ephemeral node ，比如/brokers/ids/1，发生 session timeout； 

当然发生这个的原因，除了server dead，还有很多，比如网络不通；但是我们不关心，只要出现 sessioin timeout，我们就认为这个 broker 不工作了； 
会出现如下的log,

当一个 broker failure 会影响什么，其实对于多 replicas 场景，一般对最终客户没啥影响


只会影响哪些 leader replica 在该 broker 的 partitions； 需要重新做 leader election，
如果无法选出一个新的 leader，会导致 partition offline。 

因为如果只是 follow replica failure，不会影响 partition 的状态，还是可以服务的，只是可
用 replica 少了一个；需要注意的是，kafka 是不会自动补齐失败的replica的，即坏一个少一个； 

但是对于 leader replica failure，就需要重新再 elect leader，前面已经讨论过，新选取出的 leader 是
可以保证 offset 一致性的；

Note： 其实这里的一致性是有前提的，即除了 fail 的 leader，在 ISR（in-sync replicas） 里面还存在其他的 replica；
顾名思义，**ISR，就是能 catch up with leader 的 replica**。 

虽然 partition 在创建的时候，会分配一个 AR（assigned replicas），但是在运行的过程中，可能会有一些 replica 由于各种原因无法跟上 leader，这样的 replica 会被从 ISR 中去除。 

所以 ISR <= AR； 

如果，ISR 中 没有其他的 replica，并且允许 unclean election，那么可以从 AR 中选取一个 leader，但这样一定是丢数据的，无法保证 offset 的一致性。

**Broker Startup**

这里的 startup，就是指 failover 中的 startup，会出现如下的log，

过程也不复杂，先将该 broker 上的所有的 replica 设为 online，然后触发 offline partition 或 new partition 
的 state 转变为 online 

所以 broker startup，只会影响 offline partition 或 new partition，**让他们有可能成为 online**

那么对于普通的已经 online partition，影响只是多一个可用的 replica，那还是在它完成catch up，被加入 ISR 后的事

### Controller failover

前面说明过，某个 broker server 会被选出作为 Controller，这个选举的过程就是依赖于 zookeeper 
的 ephemeral node，谁可以先在"/controller"目录创建节点，谁就是 controller； 

所以反之，我们也是 watch 这个目录来判断 Controller 是否发生 failover 或 变化。Controller 发
生 failover 时，会出现如下 log

```
controller.log： 
“INFO [SessionExpirationListener on 1], ZK expired; shut down all controller 
components and try to re-elect (kafka.controller.KafkaController$SessionExpirationListener)”
```

Controller 主要是作为 master 来仲裁 partition 的 leader 的，并维护 partition 和 replicas 的状态机，
以及相应的 zk 的 watcher 注册；

Controller 的 failover 过程如下：

* 试图去在“/controller” 目录抢占创建 ephemeral node；
* 如果已经有其他的 broker 先创建成功，那么说明新的 controller 已经诞生，更新当前的元数据即可；
* 如果自己创建成功，说明我已经成为新的 controller，下面就要开始做初始化工作，
* 初始化主要就是创建和初始化 partition 和 replicas 的状态机，并对 partitions 和 brokers 的目录的
  变化设置 watcher。

可以看到，单纯 Controller 发生 failover，是不会影响正常数据读写的，只是 partition 的 leader 无法被重
新选举，如果此时有 partition 的 leader fail，会导致 partition offline; 

但是 Controller 的 dead，往往是伴随着 broker 的 dead，所以在 Controller 发生 failover 的过程中，往往
会出现 partition offline， 导致数据暂时不可用

### 数据同步 

Producer.required.acks 设置

```
acks = 0，发就发了，不需要 ack，无论成功与否
acks = 1，当写 leader replica 成功后就返回，其他的 replica 都是通过fetcher去异步更新的，
  当然这样会有数据丢失的风险，如果leader的数据没有来得及同步，leader挂了，那么会丢失数据
acks = –1, 要等待所有的replicas都成功后，才能返回; 这种纯同步写的延迟会比较高  
```

所以，一般的情况下，thoughput 优先，设成1，在极端情况下，是有可能丢失数据的 
如果可以接受较长的写延迟，可以选择将 acks 设为 –1

kafka 的 log patition 存在文件中，并以 offset 作为索引，所以 consumer 需要对于每个 partition 记录
上次读到的 offset （high-level和low-level的区别在于是 kafka 帮你记，还是你自己记)

所以如果 consumer dead，重启后只需要继续从上次的 offset 开始读，那就不会有不一致的问题

但如果是 Kafka broker dead，并发生 partition leader 切换，如何保证在新的 leader 上这个 offset 仍然有效

![](http://images2015.cnblogs.com/blog/312753/201511/312753-20151118110739796-1382433538.png)

log 除了有 log end offset 来表示 log 的末端，还有一个 committed offset， 表示有效的 offset； 
committed offset 只有在所有 replica 都同步完该 offset 后，才会被置为该offset； 

所以图中 committed 置为2， 因为 broker3 上的 replica 还没有完成 offset 3 的同步； 
所以这时，offset 3 的 message 对 consumer 是不可见的，consumer最多只能读到 offset 2。 
如果此时，leader dead，无论哪个 follower 重新选举成 leader，都不会影响数据的一致性，因为consumer可见的offset最多为2，而这个offset在所有的replica上都是一致的。

所以在一般正常情况下，当 kafka 发生 failover 的时候，consumer 是不会读到不一致数据的。特例的情况就是，
当前 leader 是唯一有效的 replica，其他replica都处在完全不同步状态，这样发生 leader 切换，
一定是会丢数据的，并会发生 offset 不一致。

### zookeeper failover

如果 zookeeper dead，broker 是无法启动的，报如下的异常：

这种异常，有可能是 zookeeper dead，也有可能是网络不通，总之就是连不上 zookeeper。 
这种 case，kafka完全不工作，直到可以连上 zookeeper 为止。

**Zookeeper Hang**

其实上面这种情况比较简单，比较麻烦的是 zookeeper hang，可以说 kafka 的80%以上问题都是由于这个原因 
zookeeper hang 的原因有很多，主要是 zk 负载过重，zk 所在主机 cpu，memeory 或网络资源不够等

zookeeper hang 带来的主要问题就是 session timeout，这样会触发如下的问题，

a. Controller Fail，Controller 发生重新选举和切换，具体过程参考下文。

b. Broker Fail，导致partition的leader发生切换或partition offline，具体过程参考下文。

c. Broker 被 hang 住 。 

问题在于“The current behavior of zookeeper for ephemeral nodes is that session expiration and ephemeral node deletion is not an atomic operation.” 
即 zk 的 session 过期和 ephemeral node 删除并不是一个原子操作; 
出现的case如下：

* 在极端case下，zk 触发了 session timeout，但还没来得及完成 /brokers/ids/1 节点的删除，就被 hang 住了，比如是去做很耗时的 fsync 操作 。
* 但是 broker 1 收到 session timeout 事件后，会尝试重新去 zk 上创建 /brokers/ids/1 节点，可这时旧的节点仍然存在，所以会得到 NodeExists，其实这个是不合理的，因为既然 session timeout，这个节点就应该不存在。
* 通常的做法，既然已经存在，我就不管了，该干啥干啥去；问题是一会 zk 从 fsync hang 中恢复了，他会记得还有一个节点没有删除，这时会去把 /brokers/ids/1 节点删除。
* 结果就是对于client，虽然没有再次收到 session 过期的事件，但是 /brokers/ids/1 节点却不存在了。

所以这里做的处理是，在前面发现 NodeExists 时，while true 等待，一直等到 zk 从 hang 中恢复删除该节点，然后创建新节点成功，才算完； 

这样做的结果是这个broker也会被一直卡在这儿，等待该节点被成功创建。

### Tips

**a, 验证topic 是否work？**

最简单的方式，就是用 producer 和 consumer console 来测试

Producer console，如下可以往 localhost 的 topic test，插入两条 message，

```bash
> bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 
This is a message
This is another message

bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning
```

如果整个过程没有报错，ok，说明你的topic是可以工作的

**b, 再看看topic是否健康？**

```bash
bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic test
```

从这个图可以说明几个问题：

首先，topic 有几个 partitions，并且 replicas factor 是多少，即有几个 replica？ 

图中分别有32个 partitions，并且每个 partition 有两个 replica。

再者，每个 partition 的 replicas 都被分配到哪些 brokers 上，并且该 partition 的 leader 是谁？ 

比如，图中的 partition0，replicas 被分配到 brokers 4和1上面，其中 leader replica 在 broker 1 上。

最后，是否健康？ 

从以下几个方面依次表明健康程度,

* Isr 为空, 说明这个 partition 已经 offline 无法提供服务了，这种 case 在我们的图中没有出现；
* Isr 有数据, 但是 Isr < Replicas，这种情况下对于用户是没有感知的，但是说明有部分 replicas 已经出问题了，至少是暂时无法和 leader 同步；比如，图中的 partition0，Isr 只有1，说明 replica 4 已经 offline
* Isr = Replicas, 但是 leader 不是 Replicas 中的第一个 replica，这个说明 leader 是发生过重新选取的，这样可能会导致 brokers 负载不均衡；比如，图中的 partition9，leader是2，而不是3，说明虽然当前它的所有 replica 都是正常的，但之前发生过重新选举。


### storm kafka plugin
