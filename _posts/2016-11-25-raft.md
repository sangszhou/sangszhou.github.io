---
layout: post
title: Raft concept and implementation
categories: [distributed]
description: raft
keywords: distributed
---

Raft is a consensus algorithm designed as an alternative to Paxos. It was meant to be more understandable than Paxos by means of separation of logic, but it also formally proven safe and offers some new features. Raft offers a generic way to distribute a state machine across a cluster of computing systems, ensuring that each node in the cluster aggrees upon the same series of state transitions. 

## Basic

Raft achieve consensus via an elected leader. A server in a raft cluster is either a leader, a candidate or a follower. The leader is responsible for log replication to the followers. It regularly informs the followers of its existence by sending a heartbeat message. Each follower has a timeout(typically between 150 and 300 ms) in which it expects the heartbeat from the leader. The timeout is reset on receiving the heartbeat. If no heartbeat is received the follower changes its status to candidate and starts a new leader election.


## Leader election

A leader election is started by a candidate server. A server becomes a candidate if it receives no heartbeat from the leader within the timeout. It starts the election by increasing the term counter and sending a RequestVote message to all other servers. The other servers will vote for the first candidate that sends them a RequestVote message. A server will only vote once for a term. If the candidate receives a message from a leader with a term number equals or larger than the current term the election is defeated and the candidate changes into a follower. If a candidate receives a majority of votes then it becomes the new leader. If neither happends, because of a split vote, then a new leader election is started after a timeout. 
 
 The timeout value of each server should be spread out within a reasonable interval. This should reduce the chance a split vote because servers won't become candidate at the same time.
 
## Log replication
 
 The leader is responsible for the log replication. It accepts client requests. The requests are forwarded to the followers in AppendEntries messages. Once the leader receives confirmation from the majority of its followers the request is considered committed.
 
## Implementation
 
### Raft 和数据类型定义
 
```go
 type ApplyMsg struct {
 	Index       int
 	Command     interface{}
 	UseSnapshot bool   
 	Snapshot    []byte 
 }
 
 type LogEntry struct {
 	Term    int
 	Index   int
 	Command interface{}
 }
 
 type RequestVoteArgs struct {
 	Term         int
 	CandidateId  int
 	LastLogIndex int
 	LastLogTerm  int
 }
 
 type RequestVoteReply struct {
 
 	Term        int // current term for candidate to update his term
 	VoteGranted bool
 }
 
 type AppendEntriesArgs struct {
 	Term            int
 	LeaderId        int
 	PrevLogIndex    int
 	PrevLogTerm     int
 	Entries         []LogEntry
 	LeaderCommitted int
 }
 
 type AppendEntriesReply struct {
 	Term        int
 	Success     bool
 	Consistency bool
 }
```
 
### RequestVote
 
```go
 func (rf *Raft) sendRequestVote(server int, args RequestVoteArgs, reply *RequestVoteReply) bool {
 
 	ok := rf.peers[server].Call("Raft.RequestVote", args, reply)
 	return ok
 }
 
 func (rf *Raft) RequestVote(args RequestVoteArgs, reply *RequestVoteReply) {
 	reply.Term = rf.CurrentTerm
 
 	if args.Term < rf.CurrentTerm {
 		reply.VoteGranted = false
 		return
 	}
 
 	defer rf.persist()
 	// if current term < args.term, update vote for and change immediately
 	if args.Term > rf.CurrentTerm {
 		// go to follower role
 		rf.CurrentTerm = args.Term
 		go func() { rf.BecomeFollower <- true }()
 	}
 
 	/**
 	Raft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs.
 	If the logs have last entries with different terms, then the log with the later term is more up-to-date. If the
 	logs end with the same term, then whichever log is longer is more up-to-date
 	 */
 	if rf.VotedFor == -1 || rf.VotedFor == args.CandidateId {
 		// do not consider start
 		lastLogEntry := rf.Log[len(rf.Log) - 1]
 		if args.LastLogTerm > lastLogEntry.Term  ||
 			(lastLogEntry.Term == args.LastLogTerm &&  args.LastLogIndex >= lastLogEntry.Index) {
 
 			rf.VotedFor = args.CandidateId
 			reply.VoteGranted = true
 
 			log.Printf("server %d send vote back to candidate %d", rf.me, args.CandidateId)
 		} else {
 			log.Printf("didn't vote because candidate's term and index is not up-to-date")
 		}
 	}
 }
 ```
 
 
 
### AppendEntries
 
```go
 func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply) {
 	log.Printf("appendEntries message received at server[%d] with message info %s", rf.me, args.ToString())
 
 	reply.Term = rf.CurrentTerm
 	reply.Success = false
 	reply.Consistency = false // will go give this a default value?
 
 	if args.Term < rf.CurrentTerm {
 		return
 	}
 
    // receive heartbeat message
 	go func() {rf.HeartBeat <- true}()
 
 	if args.Term > rf.CurrentTerm {
 		rf.CurrentTerm = args.Term
 		go func() { rf.BecomeFollower <- true }() // duplicated?
 	}
 
 	changeApplied := false
 
 	for i := len(rf.Log) - 1; i >= 0; i -- {
 		entry := rf.Log[i]
 		// quick check
 		if args.PrevLogTerm > entry.Term {
 			break
 		}
 		if entry.Term == args.PrevLogTerm && entry.Index == args.PrevLogIndex {
 			// update local log
 
 			reply.Consistency = true
 			reply.Success = true
 			changeApplied = true
 
 
 			if len(args.Entries) > 0 {
 
 				rf.Log = append(rf.Log[:i + 1], args.Entries...)
 				log.Printf("update server[%d] log, new log is %s", rf.me, logInfo(rf.Log))
 			}
 			rf.mayApplyCommit(args)
 			// should persistent?
 			rf.persist()
 			break
 		}
 	}
 
 	if !changeApplied {
 		log.Println("AppendEntries(), from leader(%d) to server(%d) didnt change anything", args.LeaderId, rf.me)
 	}
 
 	//log.Printf("AppendEntries(): exit")
 }
```
 
### broadcast message
 
```go
 func (rf *Raft) broadcastEntries() {
 
 	if rf.Role != leader {
 		return
 	}
 	for i := 0; i < len(rf.peers); i ++ {
 		if i != rf.me {
 			go func(server int) {
 				appendEntriesArgs := rf.generateAppendEntryArg(server)
 				reply := AppendEntriesReply{}
 
 				for {
 					rf.sendAppendEntries(server, appendEntriesArgs, &reply)
 
 					if rf.Role != leader {
 						return
 					}
 
 					if reply.Success {
 						rf.handleAppendEntriesReply(server, appendEntriesArgs, &reply)
 						break
 					}
 
 					if  reply.Term > rf.CurrentTerm {
 						rf.CurrentTerm = reply.Term
 						rf.VotedFor = -1
 						rf.Role = follower
 						go func() {rf.BecomeFollower <- true}()
 						break
 					} else {
 						if !reply.Consistency {
 							if rf.NextIndex[server] > 1 {
 								rf.NextIndex[server] = rf.NextIndex[server] - 1
 								appendEntriesArgs = rf.generateAppendEntryArg(server)
 							} else {
 								// should not happen
 								// resent the same
 							}
 						}
 					}
 				}
 			}(i)
 		}
 
 	}
 }
```
 
```go
 func (rf *Raft) handleAppendEntriesReply(server int, args*AppendEntriesArgs, reply *AppendEntriesReply) {
 
 	log.Printf("handleAppendEntriesReply for server[%d], appendInfo is %s", server, args.ToString())
 
 	//rf.mu.Lock()
 	if len(args.Entries) > 0 {
 		rf.NextIndex[server] = args.Entries[len(args.Entries) - 1].Index + 1
 		log.Printf("self node is [%d], rf.NextIndex[%d]=%d", rf.me, server, rf.NextIndex[server])
 		rf.MatchIndex[server] = rf.NextIndex[server] - 1
 	}
 	//rf.mu.Unlock()
 
 	// may update committed index
 	if rf.MatchIndex[server] > rf.LastCommittedIndex {
 		// find min number of the biggest majority
 		tmp := make([]int, len(rf.MatchIndex))
 		copy(tmp, rf.MatchIndex)
 		sort.Ints(tmp) //increasing order
 
 		// leader is the biggest one, but it's next or match are always the smallest
 		localMin := tmp[len(tmp)/2+1]
 
 		if localMin > rf.LastCommittedIndex {
 			rf.LastCommittedIndex = localMin
 		}
 	}
 
 	// apply new commands
 	for i:= rf.LastAppliedIndex +1; i <= rf.LastCommittedIndex; i ++ {
 		log.Printf("leader[%d] apply index[%d] command to self", rf.me, i)
 		applyMsg := ApplyMsg {}
 		applyMsg.Command = getLogEntryByIndex(rf.Log, i)
 		applyMsg.Index = i
 
 		// 一个大 bug, 忘了通知 channel
 		rf.ApplyCh <- applyMsg
 		rf.LastAppliedIndex = i
 	}
 
 
 	// save all those data
 	rf.persist()
 }
```
 
```go
 func (rf *Raft) generateAppendEntryArg(server int) *AppendEntriesArgs {
 	//rf.mu.Lock()
 	//defer rf.mu.Unlock()
 
 	args := AppendEntriesArgs{}
 	args.Term = rf.CurrentTerm
 	args.LeaderId = rf.me
 	args.LeaderCommitted = rf.LastCommittedIndex
 
 	// smallest one, what's the default value?
 	//args.prevLogTerm = 0
 	//args.PrevLogIndex = 0
 	//args.Entries = rf.Log[1:] // will this throw exception?
 
 	// index 是一个连续的单调递增序列么
 	if rf.NextIndex[server] > rf.Log[len(rf.Log) - 1].Index + 1 {
 		panic("next index bigger than next index, not impossible")
 	}
 
 	log.Printf("generateAppendEntryArg(): leaader[%d] for server:[%d], nextIndex[%d] -> logEndIndex[%d]",
 		rf.me, server, rf.NextIndex[server], rf.Log[len(rf.Log) - 1].Index + 1)
 
 	if rf.NextIndex[server] > rf.LogTail().Index + 1 {
 		panic("nextIndex much bigger than log tail, should not happen")
 	} else if rf.NextIndex[server] == rf.LogTail().Index + 1 {
 		args.PrevLogTerm = rf.LogTail().Term
 		args.PrevLogIndex = rf.LogTail().Index
 		args.Entries = []LogEntry {} // empty, work as heartbeat
 	} else {
 		assigned := false
 		for i := len(rf.Log) - 1; i > 0; i -- {
 			if (rf.Log[i].Index == rf.NextIndex[server]) {
 				args.PrevLogIndex = rf.Log[i - 1].Index
 
 				// term 容易出错
 				//if len(rf.Log == 0) {
 				//	args.prevLogTerm = rf.CurrentTerm
 				//} else {
 				args.PrevLogTerm = rf.Log[i - 1].Term
 				//}
 
 				args.Entries = rf.Log[i:]
 				assigned = true
 				break
 			}
 		}
 		if assigned {
 			DPrintf("appendEntries assigned")
 		} else {
 			panic("smaller than smallest, not impossible")
 		}
 	}
 
 	return &args
 }
```
 
### leader, follower, candidate 状态
 
```go
 func (rf *Raft) leaderState() {
 
 	heartBeatInterval := 70
 	t := time.NewTimer(time.Duration(heartBeatInterval) * time.Millisecond)
 
 	rf.Role = leader
 	rf.VotedFor = -1
 
 	for rf.getRole() == leader {
 		select {
 		case <-t.C:
 		//@todo
 			rf.broadcastEntries()
 			t = time.NewTimer(time.Duration(heartBeatInterval) * time.Millisecond)
 
 		case <-rf.BecomeFollower:
 			go func() { rf.followerState() }()
 			return
 		}
 	}
 }
```
 
```go
 func (rf *Raft) followerState() {
 
 	electionTimeout := 150 // should be longer than leader broadcast， at least 3 times longer
 	randomizedTimeout := electionTimeout + rand.Intn(100) * 2
 	t := time.NewTimer(time.Duration(randomizedTimeout) * time.Millisecond)
 
 	rf.Role = follower
 	rf.VotedFor = -1
 
 	for rf.getRole() == follower {
 
 		select {
 		case <-t.C:
 			log.Printf("follower %d state timeout, enter candidate state", rf.me)
 			go func() {rf.candidateState()}()
 			return
 
 		case <-rf.HeartBeat:
 
 			randomizedTimeout := electionTimeout + rand.Intn(100) * 2
 			t.Reset(time.Duration(randomizedTimeout) * time.Millisecond)
 
 		case <-rf.BecomeFollower:
 			go func() { rf.followerState() }()
 			return
 		}
 
 	}
 }
```
 
```go
 func (rf *Raft) candidateState() {
 
 	// election timeouts are chosen randomly from a fixed interval e.g., 150–300ms
 	electionTimeout := 150
 	randomizedTimeout := electionTimeout + rand.Intn(150)
 	t := time.NewTimer(time.Duration(randomizedTimeout) * time.Millisecond)
 
 	collected := 0
 	vote := make(chan bool)
 	preempted := make(chan bool)
 
 	//rf.mu.Lock()
 	rf.Role = candidate
 	rf.CurrentTerm = rf.CurrentTerm + 1
 	rf.VotedFor = rf.me
 
 	args := RequestVoteArgs{
 		Term: rf.CurrentTerm,
 		CandidateId: rf.me,
 		LastLogTerm: rf.Log[len(rf.Log) - 1].Term,
 		LastLogIndex: rf.Log[len(rf.Log) - 1].Index,
 	}
 
 	for idx := 0; idx < len(rf.peers); idx ++ {
 		if idx != rf.me {
 			go func(server int) {
 
 				var reply RequestVoteReply // rpc 惯用法中级
 
 				rf.sendRequestVote(server, args, &reply)
 				// how to calculate the value?
 
 				// if current node is still candidate status
 				if rf.Role == candidate && reply.VoteGranted {
 					//collected += 1 // not thread safe?
 					DPrintf("candidate[%d] received granted vote from server[%d]\n", rf.me, server)
 					go func() {
 						vote <- true
 					}()
 
 				} else if !reply.VoteGranted {
 
 					if reply.Term > rf.CurrentTerm {
 						rf.CurrentTerm = reply.Term
 						rf.VotedFor = -1
 						rf.Role = follower
 						go func() {
 							preempted <- true
 						}()
 					} else {
 						DPrintf("candidate[%d] received granted failed from server[%d]\n" +
 							"because target server log inconsistency", rf.me, server)
 					}
 				}
 			}(idx)
 		}
 	}
 
 	for rf.Role == candidate {
 		select {
 		case <-vote:
 			DPrintf("vote received in server %d", rf.me)
 			collected += 1
 
 			if collected >= len(rf.peers) / 2 {
 				// current leader is us
 				go func() {rf.leaderState()}()
 				return
 			}
 		case <-t.C:
 		// time out and no leader received, should go start another round of scout
 		// reset everything
 			go func() {rf.candidateState()}()
 			return
 
 		case <- rf.HeartBeat:
 			log.Printf("receive heart beat in candidate position")
 			go func() {rf.followerState()}()
 			return
 
 		case <-preempted:
 		// go to
 			go func() {rf.followerState()}()
 			return
 
 		case <-rf.BecomeFollower:
 			go func() {rf.followerState()}()
 			return
 		}
 	}
 }
```