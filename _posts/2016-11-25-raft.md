---
layout: post
title: Raft concept and implementation
categories: [distributed]
description: raft
keywords: distributed
---

**Update: 2017年02月13日 星期一**

**角色**

candidate, follower, leader

**VotedFor 在一个 Term 上只能有一个为什么？**

为什么 Paxos 可以 vote 多次？因为 Paxos 是没有 Leader 概念的，而 Raft 是有的。如果一个 term 中产生了两个 Leader 可能会有问题，所以干脆不让 votedFor 出现两次

term 从 0 开始单调递增的

**如果一个节点失去了网络连接，那么它必然会把 Term num +1, 那他岂不是会更新整个集群的 Term number?**

**nextIndex 和 matchIndex 分别是干什么的? 他们俩能不能合并成一个呢？**

```go
 	if len(args.Entries) > 0 {
 		rf.NextIndex[server] = args.Entries[len(args.Entries) - 1].Index + 1
 		log.Printf("self node is [%d], rf.NextIndex[%d]=%d", rf.me, server, rf.NextIndex[server])
 		rf.MatchIndex[server] = rf.NextIndex[server] - 1
 	}
```

matchIndex and nextIndex will consistence once follower replies that the appendEntry is accepted, before that nextIndex will be bigger than matchIndex

nextIndex 从最大的开始，每次减一。matchIndex 最开始为 0.

感觉能够合成为一个。首先 matchIndex 记录的是 follower 和 leader 共同的部分，next 是记录 follower 期待的数据。在 appendEntries 返回的时候，matchIndex = nextIndex -1。这个时候他俩是一致的。再次发送 appendEntries 时，使用 nextIndex 确定 preLogIndex 和 preLogTerm，这个时候 matchIndex 正好是 nextIndex-1，所以 nextIndex 本身没有带来比 matchIndex 更多的信息量，所以没问题。


**为什么要推选 commit 比较长的节点作为新 leader**

首先，比较长的是 commit 而不是所有的日志，因为 raft 的安全性保证是不会回滚已经被 commit 的日志。而被 commit 就表示这个日志被大多数节点所接受了，如果一个节点想成为 leader 他必须获得大多数节点的反馈，这个时候，节点就有重合，那么肯定能选出那个 commit 历史比较长的节点作为 leader。

**Update: 2016年12月31日 星期六**

1. 代码部分有些 bug, select 结构体内最好使用 listener <-, 在代码的其他部分使用 listener ->, 因为这两个符号的方向并不是完全等价的。 -> 在 channel close 以后不会报错，而 <- 会
2. 文章没有探讨 raft 的实现过程，只给出了代码，这部分要补全. 当然论文已经写得很清楚了
3. there is an article written about how to do the mit 6.824 lab which is very helpful if you want to write a fully functional raft implementation


Raft is a consensus algorithm designed as an alternative to Paxos. It was meant to be more understandable than Paxos by means of separation of logic, but it also formally proven safe and offers some new features. Raft offers a generic way to distribute a state machine across a cluster of computing systems, ensuring that each node in the cluster aggrees upon the same series of state transitions. 

Update:

定理证明:

Election safety: at most one leader can be elected in a given term.

Leader append-only: a leader never overwrites or deletes entries in its log, it only append new entries.

Log Matching: If two logs contains an entry with same index and term, then the logs are identical in all entries up through the given index.

Leader completeness: if a log entry is committed in a given term, then that entry will be present in the logs of the leaders for all higher-numbered term.

State Machine Safety: if a server has applied a log entry at a given index to its state machine, no other server will ever apply a different log entry for the same entry.

如何保证最后一条，即 log 正确？如果一个 leader 发送了 appendEntries 到少量机器，然后挂掉了，leader 选举后，新 leader 又发了两个消息，然后又挂掉了，

appendEntries 会记录每个 follower 机器上的 log 信息， 发送 append 请求时，会把 log 的上一条内容附带上，也就是建立一个绑定关系，有了这个绑定关系



**Election Restrictions**



**Raft 怎么保证安全性的**

1. 推选 leader 时，选择那个 commit index 最高的那个，这样的话比较简单，且能够保证 data flow 总是从 leader 到 follower. 这样 leader 不会改动 log。论文里，原话是， candidate cannot be elected unless it has all the committed entries.


**Committing entries from previous term**

这个小节是 raft 比较难懂的一点。这句话是说，raft 协议不会认可那些比较长但是还没有被 commit 的 log, 这是因为 "raft make it easy to reason about log entries." 牺牲效率（丢弃很多 log) 但是容易理解。


**为什么 commitIndex 和 appliedIndex 不需要持久化**

这个 field 比较特殊，相比较于其他的 field (term, votedFor(), id). 首先，作者说这两个 field 不需要持久化是有一个假定的，这个假定就是 replica state machine 是驻留内存的，不会持久到磁盘，在这种情况下，当 leader 启动以后，可以根据 follower 的信息重构出 committedIndex, 然后再一个个的 apply 到 rsm 上。如果持久到了 mysql, 那个这个假设就不再成立了，那个时候，可能就得把这两个 field 也保存一下。

**和 Kafka 的丢 log 相比，有什么区别**

首先，raft 不会丢弃已经被 committed 的 logEntry. 好像 kafka 和 raft 面临同样的问题。


## Basic

Raft achieve consensus via an elected leader. A server in a raft cluster is either a leader, a candidate or a follower. The leader is responsible for log replication to the followers. It regularly informs the followers of its existence by sending a heartbeat message. Each follower has a timeout(typically between 150 and 300 ms) in which it expects the heartbeat from the leader. The timeout is reset on receiving the heartbeat. If no heartbeat is received the follower changes its status to candidate and starts a new leader election.


## Leader election

A leader election is started by a candidate server. A server becomes a candidate if it receives no heartbeat from the leader within the timeout. It starts the election by increasing the term counter and sending a RequestVote message to all other servers. The other servers will vote for the first candidate that sends them a RequestVote message. A server will only vote once for a term. If the candidate receives a message from a leader with a term number equals or larger than the current term the election is defeated and the candidate changes into a follower. If a candidate receives a majority of votes then it becomes the new leader. If neither happends, because of a split vote, then a new leader election is started after a timeout. 
 
 The timeout value of each server should be spread out within a reasonable interval. This should reduce the chance a split vote because servers won't become candidate at the same time.
 
## Log replication
 
 The leader is responsible for the log replication. It accepts client requests. The requests are forwarded to the followers in AppendEntries messages. Once the leader receives confirmation from the majority of its followers the request is considered committed.

## How it works

**Notes:** There are few points need to be noted specifically, because these points brings a lot of trouble when learning/implementing raft protocol

1. Leader will try to send appendEntries to followers, if appendEntries is not match with follower, leader will simply decrease the appendEntry argument and retry
2. follower will vote for candidate only once in one term, and it will persistent the votedFor value. There are few values need to be persisted than you might thought. Few values you thought need to persistent actually not need to and few value you thought unimportant turned out very important and need to be persisted.
3. matchIndex and nextIndex will consistence once follower replies that the appendEntry is accepted, before that nextIndex will be bigger than matchIndex


## Deal with failures and inconsistency

Besides `FAQ and updates` part, consensus algorithm deal with failure is very interesting.  Every consensus algorithm summary need have one chapter talk about how it deal with failures and inconsistency.



## Implementation
 
<<<<<<< HEAD
## Implementation
 
=======
>>>>>>> 05c61bb97727812993eced7115a410dca949139b
### Raft 和数据类型定义
 
```go
 type ApplyMsg struct {
 	Index       int
 	Command     interface{}
 	UseSnapshot bool   
 	Snapshot    []byte 
 }
 
 type LogEntry struct {
 	Term    int
 	Index   int
 	Command interface{}
 }
 
 type RequestVoteArgs struct {
 	Term         int
 	CandidateId  int
 	LastLogIndex int
 	LastLogTerm  int
 }
 
 type RequestVoteReply struct {
 
 	Term        int // current term for candidate to update his term
 	VoteGranted bool
 }
 
 type AppendEntriesArgs struct {
 	Term            int
 	LeaderId        int
 	PrevLogIndex    int
 	PrevLogTerm     int
 	Entries         []LogEntry
 	LeaderCommitted int
 }
 
 type AppendEntriesReply struct {
 	Term        int
 	Success     bool
 	Consistency bool
 }
```
 
### RequestVote
 
```go
 func (rf *Raft) sendRequestVote(server int, args RequestVoteArgs, reply *RequestVoteReply) bool {
 
 	ok := rf.peers[server].Call("Raft.RequestVote", args, reply)
 	return ok
 }
 
 func (rf *Raft) RequestVote(args RequestVoteArgs, reply *RequestVoteReply) {
 	reply.Term = rf.CurrentTerm
 
 	if args.Term < rf.CurrentTerm {
 		reply.VoteGranted = false
 		return
 	}
 
 	defer rf.persist()
 	// if current term < args.term, update vote for and change immediately
 	if args.Term > rf.CurrentTerm {
 		// go to follower role
 		rf.CurrentTerm = args.Term
 		// 这个 voteFor 是不是和一个 term 绑定的呢？是的。可以查看 broadcast 函数，里面有把 votedFor 设置成 -1
 		go func() { rf.BecomeFollower <- true }()
 	}
 
 	/**
 	Raft determines which of two logs is more up-to-date by comparing the index and term of the last entries in the logs.
 	If the logs have last entries with different terms, then the log with the later term is more up-to-date. If the
 	logs end with the same term, then whichever log is longer is more up-to-date
 	 */
 	if rf.VotedFor == -1 || rf.VotedFor == args.CandidateId {
 		// do not consider start
 		lastLogEntry := rf.Log[len(rf.Log) - 1]
 		if args.LastLogTerm > lastLogEntry.Term  ||
 			(lastLogEntry.Term == args.LastLogTerm &&  args.LastLogIndex >= lastLogEntry.Index) {
 
 			rf.VotedFor = args.CandidateId
 			reply.VoteGranted = true
 
 			log.Printf("server %d send vote back to candidate %d", rf.me, args.CandidateId)
 		} else {
 			log.Printf("didn't vote because candidate's term and index is not up-to-date")
 		}
 	}
 }
```
 
 
 
### AppendEntries
 
```go
 func (rf *Raft) AppendEntries(args *AppendEntriesArgs, reply *AppendEntriesReply) {
 	log.Printf("appendEntries message received at server[%d] with message info %s", rf.me, args.ToString())
 
 	reply.Term = rf.CurrentTerm
 	reply.Success = false
 	reply.Consistency = false // will go give this a default value?
 
 	if args.Term < rf.CurrentTerm {
 		return
 	}
 
    // receive heartbeat message
 	go func() {rf.HeartBeat <- true}()
 
 	if args.Term > rf.CurrentTerm {
 		rf.CurrentTerm = args.Term
 		go func() { rf.BecomeFollower <- true }() // duplicated?
 	}
 
 	changeApplied := false
 
 	for i := len(rf.Log) - 1; i >= 0; i -- {
 		entry := rf.Log[i]
 		// quick check
 		if args.PrevLogTerm > entry.Term {
 			break
 		}
 		if entry.Term == args.PrevLogTerm && entry.Index == args.PrevLogIndex {
 			// update local log
 
 			reply.Consistency = true
 			reply.Success = true
 			changeApplied = true
 
 
 			if len(args.Entries) > 0 {
 
 				rf.Log = append(rf.Log[:i + 1], args.Entries...)
 				log.Printf("update server[%d] log, new log is %s", rf.me, logInfo(rf.Log))
 			}
 			rf.mayApplyCommit(args)
 			// should persistent?
 			rf.persist()
 			break
 		}
 	}
 
 	if !changeApplied {
 		log.Println("AppendEntries(), from leader(%d) to server(%d) didnt change anything", args.LeaderId, rf.me)
 	}
 
 	//log.Printf("AppendEntries(): exit")
 }
```
 
### broadcast message
 
```go
 func (rf *Raft) broadcastEntries() {
 
 	if rf.Role != leader {
 		return
 	}
 	for i := 0; i < len(rf.peers); i ++ {
 		if i != rf.me {
 			go func(server int) {
 				appendEntriesArgs := rf.generateAppendEntryArg(server)
 				reply := AppendEntriesReply{}
 
 				for {
 					rf.sendAppendEntries(server, appendEntriesArgs, &reply)
 
 					if rf.Role != leader {
 						return
 					}
 
 					if reply.Success {
 						rf.handleAppendEntriesReply(server, appendEntriesArgs, &reply)
 						break
 					}
 
 					if  reply.Term > rf.CurrentTerm {
 						rf.CurrentTerm = reply.Term
 						rf.VotedFor = -1
 						rf.Role = follower
 						go func() {rf.BecomeFollower <- true}()
 						break
 					} else {
 						if !reply.Consistency {
 							if rf.NextIndex[server] > 1 {
 								rf.NextIndex[server] = rf.NextIndex[server] - 1
 								appendEntriesArgs = rf.generateAppendEntryArg(server)
 							} else {
 								// should not happen
 								// resent the same
 							}
 						}
 					}
 				}
 			}(i)
 		}
 
 	}
 }
```
 
```go
 func (rf *Raft) handleAppendEntriesReply(server int, args*AppendEntriesArgs, reply *AppendEntriesReply) {
 
 	log.Printf("handleAppendEntriesReply for server[%d], appendInfo is %s", server, args.ToString())
 
 	//rf.mu.Lock()
 	if len(args.Entries) > 0 {
 		rf.NextIndex[server] = args.Entries[len(args.Entries) - 1].Index + 1
 		log.Printf("self node is [%d], rf.NextIndex[%d]=%d", rf.me, server, rf.NextIndex[server])
 		rf.MatchIndex[server] = rf.NextIndex[server] - 1
 	}
 	//rf.mu.Unlock()
 
 	// may update committed index
 	if rf.MatchIndex[server] > rf.LastCommittedIndex {
 		// find min number of the biggest majority
 		tmp := make([]int, len(rf.MatchIndex))
 		copy(tmp, rf.MatchIndex)
 		sort.Ints(tmp) //increasing order
 
 		// leader is the biggest one, but it's next or match are always the smallest
 		localMin := tmp[len(tmp)/2+1]
 
 		if localMin > rf.LastCommittedIndex {
 			rf.LastCommittedIndex = localMin
 		}
 	}
 
 	// apply new commands
 	for i:= rf.LastAppliedIndex +1; i <= rf.LastCommittedIndex; i ++ {
 		log.Printf("leader[%d] apply index[%d] command to self", rf.me, i)
 		applyMsg := ApplyMsg {}
 		applyMsg.Command = getLogEntryByIndex(rf.Log, i)
 		applyMsg.Index = i
 
 		// 一个大 bug, 忘了通知 channel
 		rf.ApplyCh <- applyMsg
 		rf.LastAppliedIndex = i
 	}
 
 
 	// save all those data
 	rf.persist()
 }
```
 
```go
 func (rf *Raft) generateAppendEntryArg(server int) *AppendEntriesArgs {
 	//rf.mu.Lock()
 	//defer rf.mu.Unlock()
 
 	args := AppendEntriesArgs{}
 	args.Term = rf.CurrentTerm
 	args.LeaderId = rf.me
 	args.LeaderCommitted = rf.LastCommittedIndex
 
 	// smallest one, what's the default value?
 	//args.prevLogTerm = 0
 	//args.PrevLogIndex = 0
 	//args.Entries = rf.Log[1:] // will this throw exception?
 
 	// index 是一个连续的单调递增序列么
 	if rf.NextIndex[server] > rf.Log[len(rf.Log) - 1].Index + 1 {
 		panic("next index bigger than next index, not impossible")
 	}
 
 	log.Printf("generateAppendEntryArg(): leaader[%d] for server:[%d], nextIndex[%d] -> logEndIndex[%d]",
 		rf.me, server, rf.NextIndex[server], rf.Log[len(rf.Log) - 1].Index + 1)
 
 	if rf.NextIndex[server] > rf.LogTail().Index + 1 {
 		panic("nextIndex much bigger than log tail, should not happen")
 	} else if rf.NextIndex[server] == rf.LogTail().Index + 1 {
 		args.PrevLogTerm = rf.LogTail().Term
 		args.PrevLogIndex = rf.LogTail().Index
 		args.Entries = []LogEntry {} // empty, work as heartbeat
 	} else {
 		assigned := false
 		for i := len(rf.Log) - 1; i > 0; i -- {
 			if (rf.Log[i].Index == rf.NextIndex[server]) {
 				args.PrevLogIndex = rf.Log[i - 1].Index
 
 				// term 容易出错
 				//if len(rf.Log == 0) {
 				//	args.prevLogTerm = rf.CurrentTerm
 				//} else {
 				args.PrevLogTerm = rf.Log[i - 1].Term
 				//}
 
 				args.Entries = rf.Log[i:]
 				assigned = true
 				break
 			}
 		}
 		if assigned {
 			DPrintf("appendEntries assigned")
 		} else {
 			panic("smaller than smallest, not impossible")
 		}
 	}
 
 	return &args
 }
```
 
### leader, follower, candidate 状态
 
```go
 func (rf *Raft) leaderState() {
 
 	heartBeatInterval := 70
 	t := time.NewTimer(time.Duration(heartBeatInterval) * time.Millisecond)
 
 	rf.Role = leader
 	rf.VotedFor = -1
 
 	for rf.getRole() == leader {
 		select {
 		case <-t.C:
 		//@todo
 			rf.broadcastEntries()
 			t = time.NewTimer(time.Duration(heartBeatInterval) * time.Millisecond)
 
 		case <-rf.BecomeFollower:
 			go func() { rf.followerState() }()
 			return
 		}
 	}
 }
```
 
```go
 func (rf *Raft) followerState() {
 
 	electionTimeout := 150 // should be longer than leader broadcast， at least 3 times longer
 	randomizedTimeout := electionTimeout + rand.Intn(100) * 2
 	t := time.NewTimer(time.Duration(randomizedTimeout) * time.Millisecond)
 
 	rf.Role = follower
 	rf.VotedFor = -1
 
 	for rf.getRole() == follower {
 
 		select {
 		case <-t.C:
 			log.Printf("follower %d state timeout, enter candidate state", rf.me)
 			go func() {rf.candidateState()}()
 			return
 
 		case <-rf.HeartBeat:
 
 			randomizedTimeout := electionTimeout + rand.Intn(100) * 2
 			t.Reset(time.Duration(randomizedTimeout) * time.Millisecond)
 
 		case <-rf.BecomeFollower:
 			go func() { rf.followerState() }()
 			return
 		}
 
 	}
 }
```
 
```go
 func (rf *Raft) candidateState() {
 
 	// election timeouts are chosen randomly from a fixed interval e.g., 150–300ms
 	electionTimeout := 150
 	randomizedTimeout := electionTimeout + rand.Intn(150)
 	t := time.NewTimer(time.Duration(randomizedTimeout) * time.Millisecond)
 
 	collected := 0
 	vote := make(chan bool)
 	preempted := make(chan bool)
 
 	//rf.mu.Lock()
 	rf.Role = candidate
 	rf.CurrentTerm = rf.CurrentTerm + 1
 	rf.VotedFor = rf.me
 
 	args := RequestVoteArgs{
 		Term: rf.CurrentTerm,
 		CandidateId: rf.me,
 		LastLogTerm: rf.Log[len(rf.Log) - 1].Term,
 		LastLogIndex: rf.Log[len(rf.Log) - 1].Index,
 	}
 
 	for idx := 0; idx < len(rf.peers); idx ++ {
 		if idx != rf.me {
 			go func(server int) {
 
 				var reply RequestVoteReply // rpc 惯用法中级
 
 				rf.sendRequestVote(server, args, &reply)
 				// how to calculate the value?
 
 				// if current node is still candidate status
 				if rf.Role == candidate && reply.VoteGranted {
 					//collected += 1 // not thread safe?
 					DPrintf("candidate[%d] received granted vote from server[%d]\n", rf.me, server)
 					go func() {
 						vote <- true
 					}()
 
 				} else if !reply.VoteGranted {
 
 					if reply.Term > rf.CurrentTerm {
 						rf.CurrentTerm = reply.Term
 						rf.VotedFor = -1
 						rf.Role = follower
 						go func() {
 							preempted <- true
 						}()
 					} else {
 						DPrintf("candidate[%d] received granted failed from server[%d]\n" +
 							"because target server log inconsistency", rf.me, server)
 					}
 				}
 			}(idx)
 		}
 	}
 
 	for rf.Role == candidate {
 		select {
 		case <-vote:
 			DPrintf("vote received in server %d", rf.me)
 			collected += 1
 
 			if collected >= len(rf.peers) / 2 {
 				// current leader is us
 				go func() {rf.leaderState()}()
 				return
 			}
 		case <-t.C:
 		// time out and no leader received, should go start another round of scout
 		// reset everything
 			go func() {rf.candidateState()}()
 			return
 
 		case <- rf.HeartBeat:
 			log.Printf("receive heart beat in candidate position")
 			go func() {rf.followerState()}()
 			return
 
 		case <-preempted:
 		// go to
 			go func() {rf.followerState()}()
 			return
 
 		case <-rf.BecomeFollower:
 			go func() {rf.followerState()}()
 			return
 		}
 	}
 }
```