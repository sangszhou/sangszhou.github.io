**0.9.0**

### FAQ

如何回溯过去?


### Controllers

how it works, 

### Broker HA 机制

![](/images/posts/kafka/broker_ha.png)

从图中我们可以看出HA的缓存分为生产缓存事件池和拉取缓存事件池两块结构相同的缓存区，分别缓存生产和拉取请求 

2个缓存事件池的作用:

1. 生产缓存事件池：当生产者设置了等待从partition的同步选项(requiredAcks为-1)时才会启动生产缓存。
   因为每一批生产的消息，需要等待所有的处于同步状态的从partition（in-sync）同步成功，在所有follow partition上报自
   己的水位线追上leader partition之前，生产请求会一直保留在生产缓存中，等待直到超时。
2. 拉取缓存事件池：拉取请求为什么也需要缓存？因为kafka在消费消息时有一个默认选项，一次拉取最低消费1条消息。
   那么，如果消费者拉取的时候没有任何新消息生产，则拉取请求会保留到拉取缓存中，等待直到超时。这一定程度上避免了
   反复拉取一批空消息占用带宽资源的问题，不过也把Kafka的ha缓存架构的复杂度提升了一个等级。


## 几个小问题 @todo 源码

### Partition Recovery 机制

每个Partition会在磁盘记录一个RecoveryPoint, 记录已经flush到磁盘的最大offset。
当broker fail 重启时,会进行loadLogs。 首先会读取该 Partition 的 RecoveryPoint,
找到包含RecoveryPoint的segment及以后的segment, 这些segment就是可能没有完全flush到磁盘segments
然后调用segment的recover,重新读取各个segment的msg,并重建索引

[好像和我理解的不同, 我理解的是 logsegments 只会恢复数据, 读数据应该还是去 Leader 中获取把]

partition 在文件系统中的存储
```
ls data/kafka/kafka_data

__consumer_offsets-0                    iacallhome-0                            test2-2
__consumer_offsets-1                    icam_kafka_raw-1                        test2-3
__consumer_offsets-10

cd test2-2

00000000001050803083.index  00000000001072557976.index  00000000001093802376.index
00000000001050803083.log    00000000001072557976.log    00000000001093802376.log
```

这里面的每个 log 对应的都是一个 logsegments

Log 启动与加载

```scala
// log 自带 recoveryPoint
class Log(val dir: File,
          @volatile var config: LogConfig,
          @volatile var recoveryPoint: Long = 0L,
          scheduler: Scheduler,
          time: Time = SystemTime) extends Logging with KafkaMetricsGroup {
// how log created when broker start up?

private def loadSegments() {
    // 第一遍 pass 删除非法文件, 对于 swap 文件特殊处理
    // 第二遍 pass 加载 segments
    } else if(filename.endsWith(LogFileSuffix)) {
        val start = filename.substring(0, filename.length - LogFileSuffix.length).toLong
        val indexFile = Log.indexFilename(dir, start)
        val segment = new LogSegment(dir = dir, startOffset = start, fileAlreadyExists = true)
        segments.put(start, segment)            
    
    // recover 处于 swap 状态的 log
    for (swapFile <- swapFiles) {
        val swapSegment = new LogSegment(new FileMessageSet(file = swapFile),
        swapSegment.recover(config.maxMessageSize)
```

文件也并没有加载到内存, 只是在内存中维护了一个这个东西, 表示文件存在而已

```scala
// 一个 Message 的格式
* A message. The format of an N byte message is the following:
 *
 * 1. 4 byte CRC32 of the message
 * 2. 1 byte "magic" identifier to allow format changes, value is 0 currently
 * 3. 1 byte "attributes" identifier to allow annotations on the message independent of the version (e.g. compression enabled, type of codec used)
 * 4. 4 byte key length, containing length K
 * 5. K byte key
 * 6. 4 byte payload length, containing length V
 * 7. V byte payload
 *
 * Default constructor wraps an existing ByteBuffer with the Message object with no change to the contents.
 

logSegment.recover: crc 校验

def recover(maxMessageSize: Int): Int = {
    val iter = log.iterator(maxMessageSize)
    while(iter.hasNext) {
        entry.message.ensureValid() // 如果不正常就报错, 确认方式是 checksum == computeChecksum, crc 循环冗余校验
        if(validBytes - lastIndexEntry > indexIntervalBytes) // 此 Message 大于一个 indexInterval, index 添加一项纪录
            index.append(startOffset, validBytes)
            lastIndexEntry = validBytes
        validBytes += MessageSet.entrySize(entry.message)
```

![](/images/posts/kafka/kafka_message_format.png) 

左边的是 index, 右边的是 log, log 是由 message 组成的

此外, 每个 Log (一个 Partition) 由很多 logsegment 组成

当 Partition 启动的时候, 会 recoveryPointCheckPoints


**优点**

1. 以segment为单位管理Partition数据,方便数据生命周期的管理,删除过期数据简单
2. 在程序崩溃重启时,加快recovery速度,只需恢复未完全flush到磁盘的segment
3. 通过index中offset与物理偏移映射,用二分查找能快速定位msg,并且通过分多个Segment,每个index文件很小,查找速度更快。

数据结构

```scala
Parititon: Data structure that represents a topic partition. The leader maintains the AR, ISR, CUR, RAR
本身不含数据, 持有 ReplicaManager 引用, 保存 Partition 状态
    ReplicaManager: 持有 logManager 引用, 纪录HW, 可以 appendMessage, 代表一个 broker, 可以作为 leader
        LogManager: 纪录 data dir 下的所有文件, 纪录 Partition -> Log 的对应关系, 可以增删查改 Log
            Log: 和文件打交道了
    
```

LogManager 负责 recovery

```scala
LogManager.startup
def startup() {
      scheduler.schedule("kafka-recovery-point-checkpoint",
                         checkpointRecoveryPointOffsets,
                         delay = InitialTaskDelayMs,
                         period = flushCheckpointMs,
                         TimeUnit.MILLISECONDS)    
  def checkpointRecoveryPointOffsets() 
    this.logDirs.foreach(checkpointLogsInDir)

  private def checkpointLogsInDir(dir: File): Unit =
    val recoveryPoints = this.logsByDir.get(dir.toString)
    if (recoveryPoints.isDefined) {
      this.recoveryPointCheckpoints(dir).write(recoveryPoints.get.mapValues(_.recoveryPoint))
```

recoveryPointCheckPoints 从每个 Topic 的 Log 中记录当前的 recoverPoint, 表示已经写入 disk 中的数据, 

这本是 Log 的数据, 但是 LogManager 本身也管着


### Partition Replica同步机制

1. Partition的多个replica中一个为Leader,其余为follower
2. Producer只与Leader交互,把数据写入到Leader中
3. Followers从Leader中拉取数据进行数据同步
4. Consumer只从Leader拉取数据

ISR: 所有不落后的replica集合, 不落后有两层含义:
距离上次FetchRequest的时间不大于某一个值或落后的消息数不大于某一个值, 

Leader失败后会从ISR中选取一个Follower做Leader

Replica Partition 拉取数据的过程

```scala
class ReplicaManager(val config: KafkaConfig,
    val replicaFetcherManager = new ReplicaFetcherManager(config, this, metrics, jTime, threadNamePrefix)
    // 和 leader 同步的数据
    val highWatermarkCheckpoints = config.logDirs.map(dir => (new File(dir).getAbsolutePath, 
        new OffsetCheckpoint(new File(dir, ReplicaManager.HighWatermarkFilename)))).toMap

class ReplicaFetcherThread(name: String,
                           fetcherId: Int,
                           sourceBroker: BrokerEndPoint,
                           brokerConfig: KafkaConfig,
                           replicaMgr: ReplicaManager,
    
    // 定义如何处理拉取过来的数据
    def processPartitionData(topicAndPartition: TopicAndPartition, fetchOffset: Long, partitionData: PartitionData) {
        val replica = replicaMgr.getReplica(topic, partitionId).get
        val messageSet = partitionData.toByteBufferMessageSet
        replica.log.get.append(messageSet, assignOffsets = false)
        val followerHighWatermark = replica.logEndOffset.messageOffset.min(partitionData.highWatermark)
        replica.highWatermark = new LogOffsetMetadata(followerHighWatermark)
```

上面没有提到 Replica 的信息

```scala
class Replica(val brokerId: Int,
              val partition: Partition,
              time: Time = SystemTime,
              initialHighWatermarkValue: Long = 0L,
              val log: Option[Log] = None) extends Logging {

    var highWatermarkMetadata: LogOffsetMetadata
    var logEndOffsetMetadata: LogOffsetMetadata
    
```

### 数据可靠性保证

当Producer向Leader发送数据时,可以通过 acks 参数设置数据可靠性的级别

1. 0: 不论写入是否成功,server 不需要给 Producer 发送 Response, 如果发生异常, server会终止连接,触发Producer更新meta数据;
2. 1: Leader写入成功后即发送Response,此种情况如果Leader fail,会丢失数据
3. -1: 等待所有ISR接收到消息后再给Producer发送Response,这是最强保证 
   仅设置acks=-1也不能保证数据不丢失,当Isr列表中只有Leader时,同样有可能造成数据丢失。要保证数据不丢除了设置acks=-1, 还要保 证ISR的大小大于等于2,具体参数设置:

1. request.required.acks: 设置为-1 等待所有ISR列表中的Replica接收到消息后采算写成功;
2. min.insync.replicas: 设置为大于等于2,保证ISR中至少有两个Replica 

Producer 要在吞吐率和数据可靠性之间做一个权衡

消息发送到 broker 的过程

```scala
```

### 数据一致性保证

一致性定义:若某条消息对Consumer可见,那么即使Leader宕机了,在新Leader上数据依然可以被读到

1.HighWaterMark简称HW: Partition的高水位，取一个partition对应的ISR中最小的LEO作为HW，消费者最多只能消费到HW所在的位置，
  另外每个replica都有highWatermark，leader和follower各自负责更新自己的highWatermark状态，highWatermark <= leader. LogEndOffset
2.对于Leader新写入的msg，Consumer不能立刻消费，Leader会等待该消息被所有ISR中的replica同步后,更新HW,此时该消
  息才能被Consumer消费，即Consumer最多只能消费到HW位置

这样就保证了如果Leader Broker失效,该消息仍然可以从新选举的Leader中获取。对于来自内部Broker的读取请求,没有HW的限制。
同时,Follower也会维护一份自己的HW,Fellow.HW = min(Leader.HW, Follower.offset)

### DS in zookeeper

**Broker registration info**
/brokers/ids/[brokerId]

```
Schema:
{ "fields":
    [ {"name": "version", "type": "int", "doc": "version id"},
      {"name": "host", "type": "string", "doc": "ip address or host name of the broker"},
      {"name": "port", "type": "int", "doc": "port of the broker"},
      {"name": "jmx_port", "type": "int", "doc": "port for jmx"}
      {"name": "endpoints", "type": "array", "items": "string", "doc": "endpoints supported by the broker"}
    ]
}
 
Example:
{
  "version":2,
  "host": "localhost",
  "port": 9092
  "jmx_port":9999,
  "timestamp": "2233345666",
  "endpoints": ["PLAINTEXT://host1:9092", "SSL://host1:9093"]
```

**ISR Change notification**  /isr_change_notification/isr_change_x

Gets created when ISR is changed at any broker, controller watches for these notifications and sends 
MetadataUpdateRequest to all brokers.

**Client and Topic configuration overrides: The content of both znodes has the same structure**

/config/clients/[topic_name]
/config/topics/[topic_name]

```
{
  "version": 1,
  "config": {
    "config.a": "x",
    "config.b": "y",
    ...
  }
}
```