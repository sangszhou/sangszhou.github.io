---
layout: post
title:  "Elasticsearch summary part (2/2)"
date:   "2016-09-12 01:00:00"
categories: elasticsearch
keywords: es, elasticsearch, kibana, logstash
---

## logstash

logstash 可以把 log 转成 json 数据然后保存到 ES 中

**启动 logstash**

./bin/logstash -f apache.conf

**配置文件例子**

```
input { stdin { } }

filter {
  grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
  }
  date {
    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
  }
}

output {
  elasticsearch { 
      hosts => ["localhost:9200"]
      index => "manual_visitor"
  }
  stdout { codec => rubydebug }
}
```

该配置文件定义数据的来源是 command line, 输出是 command line 和 ES server.

它用到了 grok

### grok

Grok是一个数据结构化工具。只需要通过简单地变量定义，我们就可以将文本格式的字符串，
转换成为具体的结构化的数据。Logstash [默认](https://github.com/logstash-plugins/logstash-patterns-core/blob/master/patterns/grok-patterns)
带有上百个 Grok 变量，我们可以直接使用或者稍微改写成自己的新 Grok 变量。

比如下面一条默认的 Apache日志

```
10.2.21.130 - - [08/Apr/2013:11:13:40 +0800] 
"GET /mediawiki/load.php HTTP/1.1" 304 - "http://som.d.xiaonei.com/mediawiki/index.php" 
"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_3) AppleWebKit/536.28.10 (KHTML, like Gecko) 
Version/6.0.3 Safari/536.28.10"  
```

我们使用下面这行 filter来转换。

```
filter {   
        grok {
            pattern => "%{COMBINEDAPACHELOG}"             
            type => "apache"   
} }   
```

得到的数据结构

```
{ "@source":"file://chenryn-Lenovo/home/chenryn/test.txt", 
"@tags":[],   
"@fields":{ "clientip":["10.2.21.130"], "ident":["-"], "auth":["-"], 
"timestamp":["08/Apr/2013:11:13:40 +0800"], "verb":["GET"], "request":["/mediawiki/load.php"], 
"httpversion":["1.1"], "response":["304"], 
"referrer":["\"http://som.d.xiaonei.com/mediawiki/index.php\""], 
"agent":["\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_3) AppleWebKit/536.28.10 (KHTML, like Gecko) Version/6.0.3 Safari/536.28.10\""] }, 
"@timestamp":"2013-04-08T03:34:37.959Z", 
"@source_host":"chenryn-Lenovo",   
"@source_path":"/home/chenryn/test.txt",   
"@message":"10.2.21.130 --[08/Apr/2013:11:13:40 +0800] \"GET /mediawiki/load.php HTTP/1.1\" 304 - \"http://som.d.xiaonei.com/mediawiki/ index.php\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_8_3) AppleWebKit/536.28. 10 (KHTML, like Gecko) Version/6.0.3 Safari/536.28.10\"",   
"@type":"apache" } 
```

在随后的配置和其他处理中，我们就可以随意使用这个数据结构中的内容作为变量处理了，比如 "%{clientip}" 等。

自己定义时，基本可以在这些变量基础上进行，比如 Nginx的访问日志，可以定义成以下这样。

```
NGINXURI %{URIPATH}(?:%{URIPARAM})*   
NGINXACCESS \[%{HTTPDATE}\] %{NUMBER:code:int} %{IP:client} %{HOSTNAME} %{WORD:method} %{NGINXURI:req} %{URIPROTO}/%{NUMBER:version} %{IP:upstream}(: %{POSINT:port})? %{NUMBER:upstime:float} %{NUMBER:reqtime:float} %{NUMBER:size: int} "(%{URIPROTO}://%{HOST:referer}%{NGINXURI:referer}|-)" %{QS:useragent} "(%{IP:x_forwarder_for}|-)"  
```

在写 GROK 语法时, 可以使用在线的 GROK DEBUGER 校验语法的正确性

### offset

如果一个文件已经被 logstash 处理过了, 那么再次启动 logstash 时重复的内容不会被重新读一遍,
这是因为 logstash 处理文件之前会到某个文件拿去文件的元信息, 元信息就标注了上次处理到的位置,
然后冲下一个位置开始计算。

如果不想让 logstash 记录 offset 值, 可以把 sincedb 修改成 `/dev/null`

```
input {
        file {
            path => "/ws/data/logfile"
            start_position => "beginning"
	        sincedb_path => "/dev/null"
	        type => "human_stat"
        }
}
```

这里的 type 就是 ES 的 type, index 的配置在 stdout 中

### index

如果 index 以 logstash 开头, 那么每个 field 会有 analyzed 和 not analyze 两种, 如果是自定义
的 index 则会使用默认的 mapping template 也就是全部 analyzed, 有时候这不是最好的

此外, 默认情况下, logstash 的 index 每天都会加一个, 这也是为什么创建视图是允许 index 的模糊匹配



## Kibana

创建视图选择 index 时, 这个 index 可以模糊匹配, 可以匹配一系列的图

**时间:**

右上角可以调整之间跨度, 自动刷新就是自动刷新页面

**dashboard**

dashboard 是图的集合

**visualize**

可以通过左边框选择关心的 field