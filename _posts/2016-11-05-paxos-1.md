---
layout: post
title: Paxos Concept
categories: [distributed]
description: dis
keywords: paxos summary & Implementation
---

## Paxos Concept



Update: 2017年01月22日 星期日

“The key to Paxos’ assurance of correctness lies in the interplay between requesting permission to make suggestions and actually making those suggestions. When granting permission for a new suggestion, peers must include the highest Suggestion ID and Value they have accepted, if any, in the Permission Granted message. Once the suggesting peer has received a majority of permission grants and before making its own suggestion, the peer must first examine the previously accepted Suggestion IDs and Values contained in those grants. If any peer has previously accepted a suggestion, the Value of the new suggestion must be set to the Value corresponding to the highest Suggestion ID contained in the grant messages. If and only if no peer has previously accepted a suggestion may the suggester supply an arbitrary Value of their choosing.”

大部分文章都没有讲 phase1 的作用，我认为他的作用就是不要让过去的消息来捣乱

很多分布式系统算法都可以从这个角度来分析

**Correctness**

**Ensuring forward progress**

**Learning the final result**

**Latency optimizations**

**Application-level logic**: 这是 Paxos 论文缺失的东西

More refer to [link](https://understandingpaxos.wordpress.com/)

Paxos is a family of protocols for solving consensus in a network of unreliable processors. 
Consensus is the process of agreeing on one result among a group of participants. 
This problem becomes difficult when the participants or their communication medium may experience failures.

###  假设

1. Processors operate at arbitrary speed
2. Processors may experience failure
3. Processors with stable storage may re-join the protocol after failure
4. Procesors do not collude, lie, or otherwise attempt to subvert the protocol.

1. Processors can send messages to another processor
2. Messages are sent asynchronously and may take arbitrarily long to deliver
3. Messages may be lost, reordered, or duplicated.
4. Messages are delivered without corruption.

简单来说，就是消息可能丢失，乱序，重复，延迟。进程可能会 crash, 延迟，或者 crash 之后重新加入到集群中。

### Roles

Acceptor(Voters):

The acceptor act as the fault-tolerant memory of the protocol. Acceptors are collected into groups called quorums. Any message sent to an acceptor must be sent to a quorum of acceptors.

Learner:

Learner act as the replication factor for the protocol. Once a client request has been agreed on by the acceptors, the learner may take action(execute the request and send a response to the client). To improve availability of processing, additional learner can be added.

Proposer:

A proposer advocates a client request, attempting to convince the acceptors to aggree on it, and acting as a coordinator to move the protocol forward when conflicts occur.

Paxos requires a distinguished proposer(called leader) to make progress. Many processes may believe they are leaders, but the protocol guarantees progress if one of them is eventually chosen. If two processes believe they are leaders, they may stall the protocol by continuously proposing conflicting updates. However, the safety properties are still preserved in that case.

Proposal number & aggreed value:

@todo

Non-trivality: only proposed values can be learned.

Safety: two different learners cannot learn different values.



## Basic Paxos

Each instance of basic paxos protocol on a single output value. The protocol proceeds over several rounds. A successfull round has two phase. A propose should not initate Paxos if it cannot communicate with at least a quorum of acceptors:

**Phase 1a: Prepare**

A proposer creates a proposal identified with a number N. This number must be greater than any previous proposal number used by this Proposer. Then it sends a Prepare message containing this proposal to Quorum of acceptors. The proposer decides who is in the Quorum.

从后面 multi-paxos 角度看 basic paxos, proposer number 变成了 sequence number


**Phase 1b: Promise**

If the proposal's number N is higher than any previous number received from any proposer by the acceptor, then the acceptor must return a promise to ignore all future proposals having a number less than N. If the acceptor accepted a proposal at some point in the past, it must include the previous proposal number and previous value in its response to the proposal.

otherwise, the acceptor can ignore the received proposal. It does not have to answer in this case for paxos to work. However, for the sake of optimization, sending a denial reponse would tell the proposal that it can stop its attempt to create consensus with proposal N.

**Phase 2a: Accept request**

If a Proposer receives enough promises from a Quorum of Acceptors, it needs to set a value to its proposal. If any Acceptors had previously accepted any proposal, then they'll have sent their values to the Proposer, who now must set the value of its proposal to the value associated with the highest proposal number reported by the Acceptors. If none of the Acceptors had accepted a proposal up to this point, then the Proposer may choose any value for its proposal.

**Phase 2b: Accepted**

? acceptor 之间如何相互学习？靠 leader 重发么

If an Acceptor receives an Accept Request message for a proposal N, it must accept it if and only if it has not already promised to any prepare proposals having an identifier greater than N. In this case, it should register the corresponding value v and send an Accepted message to the Proposer and every Learner. Else, it can ignore the Accept Request.

只要接受了 request, 也就默认认可了 leadership of the proposer. 

流程图

```
Client   Proposer      Acceptor     Learner
   |         |          |  |  |       |  |
   X-------->|          |  |  |       |  |  Request
   |         X--------->|->|->|       |  |  Prepare(1)
   |         |<---------X--X--X       |  |  Promise(1,{Va,Vb,Vc})
   |         X--------->|->|->|       |  |  Accept!(1,Vn)
   |         |<---------X--X--X------>|->|  Accepted(1,Vn)
   |<---------------------------------X--X  Response
   |         |          |  |  |       |  |
```

从此图可以看出，learner 是回复 client 的进程，而不是 proposer, 这就很奇怪了，为什么 client 请求的是 proposer, 而作为回复的却是 learner

几种失败场景:

1. Failure of acceptor, 少数派 acceptor 的失败，不会影响整个系统的可用性
2. Failure of redundant learner, 所有的 learner 都会通知
3. Failure of proposer(leader), 重新选举新的 leader, 假如 acceptors 接受过 accepted, 那么新的 leader 会学习到这个知识
4. dueling Proposers. 两个 proposer 会连续的推选自己 Prepare, 希望得到 acceptor 的认可，这个时候，就会发生活锁问题，这就要求
   proposer 要学会承认别人是 leader 这一事实

有一点不确定，leader 会不会学习到 accepted 这个知识呢？按照代码的说法，好像是可以的，但是没有被 accepted 的值是不应该被阻止的

```
Client   Proposer        Acceptor     Learner
   |      |             |  |  |       |  |
   X----->|             |  |  |       |  |  Request
   |      X------------>|->|->|       |  |  Prepare(1)
   |      |<------------X--X--X       |  |  Promise(1,{null,null,null})
   |      !             |  |  |       |  |  !! LEADER FAILS
   |         |          |  |  |       |  |  !! NEW LEADER (knows last number was 1)
   |         X--------->|->|->|       |  |  Prepare(2)
   |         |<---------X--X--X       |  |  Promise(2,{null,null,null})
   |      |  |          |  |  |       |  |  !! OLD LEADER recovers
   |      |  |          |  |  |       |  |  !! OLD LEADER tries 2, denied
   |      X------------>|->|->|       |  |  Prepare(2)
   |      |<------------X--X--X       |  |  Nack(2)
   |      |  |          |  |  |       |  |  !! OLD LEADER tries 3
   |      X------------>|->|->|       |  |  Prepare(3)
   |      |<------------X--X--X       |  |  Promise(3,{null,null,null})
   |      |  |          |  |  |       |  |  !! NEW LEADER proposes, denied
   |      |  X--------->|->|->|       |  |  Accept!(2,Va)
   |      |  |<---------X--X--X       |  |  Nack(3)
   |      |  |          |  |  |       |  |  !! NEW LEADER tries 4
   |      |  X--------->|->|->|       |  |  Prepare(4)
   |      |  |<---------X--X--X       |  |  Promise(4,{null,null,null})
   |      |  |          |  |  |       |  |  !! OLD LEADER proposes, denied
   |      X------------>|->|->|       |  |  Accept!(3,Vb)
   |      |<------------X--X--X       |  |  Nack(4)
   |      |  |          |  |  |       |  |  ... and so on ...
```

## Multi-paxos

If leader is relatively stable, the phase 1 becomes unnecessary. Thus, it is possible to skip future instances of 
the protocol with the same leader. 

To achieve this, the instance number I is included along with each value. Multi-Paxos reduces the failure-free message delay (proposal to learning)from 4 delays to 2 delays.

**Message flow: Multi-Paxos, start**

```
Client   Proposer      Acceptor     Learner
   |         |          |  |  |       |  | --- First Request ---
   X-------->|          |  |  |       |  |  Request
   |         X--------->|->|->|       |  |  Prepare(N)
   |         |<---------X--X--X       |  |  Promise(N,I,{Va,Vb,Vc})
   |         X--------->|->|->|       |  |  Accept!(N,I,Vm)
   |         |<---------X--X--X------>|->|  Accepted(N,I,Vm)
   |<---------------------------------X--X  Response
   |         |          |  |  |       |  |
```

Vm = highest of (Va, Vb, Vc)

在第一阶段确定了 Proposal Number 以后，后续就不用再做 Prepare 操作了。此外，I 表示 index 或者 sequence number

**Message flow: Multi-Paxos, steady-state**

```
Client   Proposer       Acceptor     Learner
   |         |          |  |  |       |  |  --- Following Requests ---
   X-------->|          |  |  |       |  |  Request
   |         X--------->|->|->|       |  |  Accept!(N,I+1,W)
   |         |<---------X--X--X------>|->|  Accepted(N,I+1,W)
   |<---------------------------------X--X  Response
   |         |          |  |  |       |  |
```

## Paxos made moderately complex

自己读过的，最容易理解的代码

在此论文中，没有了 Proposer 和 Learner, 主要是得角色变成了 Leader, Acceptor, Replica, 其中 Replica 替代了 Learner，并承接了 Proposer 的部分功能。Client 的请求直接发送给 Replica, Replica 接收到请求后会转而发给 Leader。

文章带有一个[网站](http://paxos.systems/glossary.html)，详细描述了 paxos 和他们的算法

首先，关于 paxos 涉及到的一些内容

**Consensus:** In consensus problem, each process propose some initial value, and processes that do not fail must reach an irrevocable decision on exactly one of the proposed values. In Paxos, the replicas need to agree on the next state transition they handle, so that they remain in identical state. Paxos solve the consensus problem in asynchronous environment, where each process can crash.  But still, Paxos limited by the FLP impossiblility result in its ability to reach consensus in an asynchronous environment where process can crash.

**Consistency:** Distributed system replicate shared data to improve fault tolerance, data availability and performance. However, if the updates to this state is not carefully controlled, the state on different state might converge over time, creating inconsistency between replicas. In this case, two clients that read the state from two different replicas might see two different states. Different consistency model offers different consistency guarantee, but as long as the the user follows the rules associate with a consistency level, the distributed system guarantee that the user will not observer any inconsistencies. The most intuitive type of consistency is strong consistency. A protocol supports strong consistency, if all state changes are seen by all distributed processes in same order, sequentially. Paxos support this strong consistency.

**FLP Impossibility Result:** Consensus problem is not solvable in an asynchronous system.  The result states that there exists no deterministic algorithm that solves consensus in a asynchronous environment with reliable channels if one single process will crash. This is due to the fact that it is impossible to detect if 

A replicate state machine works by having multiple state machines, also called replicas, working in parallel, maintaining same state. When the replica receive requests from client they update their state by executing the command in the request and reply to the client. This way, the state is automatically replicated by the replicas and in the event of a failure the state does not get lost, make the state machine reliable. 
 
It is easy for the replica to execute client commands in the same order and remain in sync if there is only one client or if multiple clients send their requests strictly sequential. 
 
In this example replicas receive requests from clients in the same order, execute the commands in the same order and respond to the clients. in effect staying in sync. For simplicity, it is assumed that a client can ignore duplicated messages. 

But if multiple clients send requests to replicas in parallel, the different replica might receive there requests in different orders and execute the command in different orders, causing their local states to diverge from one another over time. 

To prevent replicas from diverging in the presence of multiple clients sending requests in parallel, the order in which the client commands will be executed by replicas should be decided. 

所以，问题的重点是，当很多消息同时到达时，paxos 需要保证 RSM 中的 command 在各个节点都是一致的，这就意味着有些消息会被暂时抛掉，有些消息接受的时间较早，但是会被安排后靠后的位置上执行。 下面看看 paxos 的如何转化这个问题的:

To decide the order which the clients commands will be executed the replicas can be thought of as having a **sequence of slots** that need to be filled with commands that make up the inputs to the state machine they maintain. Each slot is indexed by a slot number, starting from 1. Replicas receive requests from clients and assign them to specific slots, create a sequence of commands. In the face of concurrently operating clients, different replicas may propose different commands for the same slot. To avoid cinconsistency, a consensus protocol chooses a single command from the proposals for every slot. In Paxos the subprotocol that implements consensus is called the multi-decree synod protocol, or just synod protocol for short. A replica awaits the decision before actually updating its senquence of commands in the table, executing the next command and computing a response to send back to the client that issues the request.

更具体些，上面的描述没有说到当发生冲突以后，两个请求只有一个会被 synod algorithm 确认，那么另一个请求 c2 占用的 slot 会被覆盖，那么 c2 怎么办呢？一般来讲，c2 不会因为自己的 slot 被占用或者与别的请求发生了冲突就被丢弃掉，实际上，他会被放到下一个可用的 slot 上，等待被下一轮 synod 算法选中。这个时候，有一个问题，加入 client 先后发出两个请求 c1, c2. c1 和 c2 能够保证在状态机上有序么？就我目前的理解，是无法保证的。假设 c1, c2 发送到了同一个 replica, 在 replica 上分别被分配了 s1, s2 两个 slot, 但是在 synod 算法中，c1 被其他的 replica 请求打败，没能成功加入到 state machine 中，这个时候 s1 对应的 slot 就是 c1' 了，持有 c1, c2 的 leader 在 c1 被击败后失去了 view, 然后他会继续获取 view, 执行 phase1, phase1 包括从 acceptor 中学习已经被 accepted 的值，从中 leader 会学到 s1 -> c1'，然后他会把 s1 -> c1 丢掉。leader 后续会被 s1 -> c1' 和 s2 -> c2 继续向 acceptor 中传递，这样 c1 的顺序就在 c2 后面了。replica 会记录那些发出的 proposal, 再没有收到 decision 时，会继续发射 proposal, 这其中就包括了 c1, 当然， c1 对应的 slot 就不再是 s1 或 s2 了，至少是 s3 起。

Essentially, the replicated state machine uses Paxos as an ordering entity which uses consensus to agree on which client command gets assigned to which slot. One has to make sure that the ordering entity itself is also reliable, that it can tolerate failures just like replicated state machine. To achieve reliability, Paxos is run by multiple specialized processes in a distributed fashion. 

How does paxos works

**Replica** very important role

1. maintain application state
2. receive requests from clients
3. asks leader to serialize the requests so all replicas see the same sequence
4. responds to clients

When a client want to execute a command cmd = <clientId, clientSeq, op>, it broadcast request message to all replicas and waits for <response, clientId, result> message from one of the replicas (all replicas will respond however) 

Replica Invariants 

R1: There are no two different commands decided for the same slot.

R2: All commands up to slot_out are in the set of decisions.

R3: slot_out cannot decrease

R4: A replica propose commands only for slots it knows the configuration for: slot_in < slot_out + WINDOW

Paxos support reconfiguration: a client can propose a special reconfiguration command, which is decided in a slot like any other command. However, if s is the index of the slot in which a new configuration is decided, it does not take effect until slot s + WINDOW. This allows up to WINDOW slots to have proposals pending --- the configuration of later slots may change. It is always possible to add new replicas --- this does not require a reconfiguration of the leaders or acceptors.

如果没有 windows 的话，一个 replica 可能会提交任意多的 request.

从代码中，可以看出几点

1. slot_in > slot_out, 因为 slot_out 是已经确认的 command, 而 slot_in 是下一个可用的 slot, 如果某个 replica 没有收到过消息，而只是被动的从 commander 线程中接受消息，那么 slot_in 可能会小于 slot_out, 但是当消息到来时，需要给 proposal 队列添加 slot_int -> command, 这个时候就会检查 slot_in 是否已经在 decision 里面了，如果是，就 +1
2. 如果 slot_in 超过了 WINDOW size, 说明此时不应该继续发送请求，但是并不是把请求丢了，而是暂时保存在 requests 队列中，以后再发送
3. decision 消息的处理函数难理解些。perform 函数要保证几件事，第一件事是 slot_out 需要

## Part II Implementation

**Paxos made moderately complex**

Update: 2017年01月22日 星期日

这里有几个问题需要注意一下，过了 2 两个月，这篇论文中的某些知识点已经忘了，只能再看一遍才能回忆出来。

首先是 slot_in 和 slot_out. slot_out 用来记录 replica 提交的 decision 中序列号最大的那个，小于 slot_in 的所有 decision 都已经被提交了，应用到了 RSM 中。 slot_in 是收到的请求，表示下一个空闲 slot 的位置。在 slot_in 和 slot_out 之间的 slot 可能已经有了 decision, 可能还没有，所以这中间是有缝隙的，所以需要两个变量来记录。

其次是 ballot_num 和 slot_num. 每个 leader 在自己启动的时候都会发射 propose 请求，当自己发送 propose 时还会把自己的，ballot_num 发出去，所以当请求都路由到一个 leader 时，paxos 的第一阶段就可以省掉了，如果发到第二个 leader, leader 会发现自己的 ballot_num 不够大了，就会更新自己的 ballot_num, 这个时候还要一次完整的 paxos 协议。

第三个问题是，为什么 paxos 分为两个阶段，第一个阶段到底是解决什么问题的？以及二阶段提交的根本问题是什么？

第四个问题是，FLP 说异步环境下无法实现一致性协议，那么 Paxos 是如何保证一致性的。

FLP 的根本问题是说，一个 process 没反应了，我们不知道是消息丢了，延迟了，还是机器除了问题

这道题目的答案应该是这样，一个大的系统中的确很难实现一个一致性协议，目前业界使用分布式一致性系统的方式都是用一个小的集群来选择一个值，一般有 5 个节点，因为节点数目比较少，所以出现问题的就很小，等于说避开了 FLP 问题。

整个系统分为几部分，第一部分是 Replica, 第二部分是 Acceptor, 第三部分是 Commander 和 Scout 以及 Leader

### Replica

Replicas can propose multiple commands for the same slot. The synod protocol chooses from these a single command
to be decided by all nonfaulty replicas. Each ballot has a unique leader. A leader can be working on arbitrary many
ballots, there must be at least f+1 leaders. Ballot are totally ordered.

```
process Replica(leaders, inital_state)
    var state := initial_state, slot_in := 1, slot_out := 1
    var requests = emptyQueue, proposals := emptyQueue, decisions := emptyQueue
    
    function propose()
        while slot_in < slot_out + WINDOW && EXIST c IN requests do
            if decisions[slot_in-WINDOW] is config
                leaders := decision[slot_in - WINDOW].leaders
            end if 
            
            if not exist c' (slot_in, c') belong to decisions then
                requests := requests - c'
                proposals := proposals + (slot_in, c')
                for all leaders, send(ballotNumber, (slot_in, c'))
            end if
            
            slot_in := slot_in + 1
        end while
        
    function perform(ballot, cid, op)
        if decisions already have op or or is config
            slot_out += 1
        else
            (next, result) = op(state)
            atomic
                state = next
                slot_out += 1
            end atomic
            send(ballot, (cid, result))
        end if
    end function
    
    
    for ever
        switch receive():
            case (request, c)
                requests += c
            case (decision, s, c)
                decisions = decisions + (s, c)
                
                while exist c` satisfy (slot_out, c`) belong to decisions do
                    if exist c`` satisfy (slot_out, c``) belong to proposals then
                           proposals = proposal - (slot_out, c``)
                           if c`` != c` then
                               requests += c``
                           end if
                    end if
                    perform(c`)
                end while
        end switch
        
        proposal()
    end for
end process
```

从源代码的角度来看，有两点难理解。第一点是 WINDOW 的用处，在论文中，Config 被视为一种特殊的 OP, Config 包含了新集群的 leaders,
acceptor, replica 信息。当 Config 消息被收到，意味着集群的信息有变，以前的 leader 可能不再是 leader, 可能多一些，可能少一些。

如果 replica 收到消息后就马上发送，那么可能会出现这么一种情况，即收到 config 的时候，acceptor 的数目增加了，从 3 -> 5, 但是
leader 收到 2 个反馈就说消息传递成功，这样就出错了。为了避免出错的可能性尽可能的小，replica 每次发出去的消息个数不超过 WINDOW 个。
当 WINDOW = 1 时，表示顺序的执行任务，每次执行更新一个
第二点是当收到 decision 后，为什么要循环变量 decision, 还要从 proposal 把某些 OP 放到 Requests 中。这是因为，
Mutli-Paxos 一次会发出去多个 Proposal, 但是这些 Proposals 被 synod protocol 决定的顺序是不能保证的。也就说，可能 slot = 3 的 decision 已经返回了，但是 Slot = 2 的 decision 还没到。perform 函数是把 decision 应用到状态机上，所以他一定要按照 slot 的顺序执行，不能有空位。比如现在 slot = 3 的 decision 已经在 queue 里，此时 decision = 2 的 decision 到了，那么就要先把 decision = 2 的操作 apply, 然后循环，apply decision = 3 的操作。因为 client 会把 OP 发送到所有的 replica 上，但是 OP 在 replica 的 slot_in 是不确定的，最好的情况是没有冲突时，所以的 OP -> slot_in 相同，那么 OP-> decision 返回时，会直接把在 Slot 的 proposal 删掉，即这个值已经确定了，replica 不要在尝试提交了，在最差的情况下，所有的 OP 对应的 slot 的都不一样，一个 OP 会被提交 N 次，这样效率就比较低了，但是多次提交的 OP 并不会被多次应用到状态机上，因为 perform 函数会检查 OP 是否已经被应用过了，如果应用过了，只增 slot_out, 不改变状态。

Scala 实现

```scala
object Replica {

  // controlled the seq
  val WINDOW = 500

  implicit val timeout = Timeout(3 second)

  case object DoProposal

  case object LookUpReplicaState

  case class ReplicaState(slotIn: Int, slotOut: Int,
                          clientInfo: Map[String, ActorRef],
                          requests: List[Command],
                          proposals: List[ProposalMessage],
                          decision: SortedMap[Int, Command],
                          rsm: Any)

  def getInternalState(replica: ActorRef): ReplicaState =
    Await.result(replica ? LookUpReplicaState, 3 second).asInstanceOf[ReplicaState]

}
```

上面是消息类型的定义，其中 rsm 也就是状态机被定义成任意类型，一般来说，他应该被设置成一个接口，这个接口可以用来作为 KVStore 什么的

getInternalState 函数是用来 debug 的，因为 actor 内部的信息看不到。下面的代码中有很多用来 debug 的，不是核心代码。

```scala
class Replica(numLeaders: Int) extends Actor with ActorLogging {

  this: ReplicateStateMachine =>

  val replicaId = "ReplicaId"

  var slot_in = 1
  var slot_out = 1

  // get leader actor selection
  val leaders: Set[ActorSelection] = (1 to numLeaders) map (i => context.actorSelection("../leader-" + i)) toSet

  //  var leaders: Set[ActorRef] = Set()
  var requests: List[Command] = List()
  var clientInfo: Map[String, ActorRef] = Map()

  var decisions: SortedMap[Int, Command] = SortedMap.empty[Int, Command]
  // better be a map
  var proposals: List[ProposalMessage] = List()

  import context.dispatcher
  context.system.scheduler.schedule(30 milli, 30 milli, self, DoProposal)


  def receive = {
    case DoProposal => propose

    /**
      * upon client request received
      * 1. check if proposals list already have the command, if so, do not send request twice
      * 2. if already too much pending requests, ignore this client request
      */
    case RequestMessage(cmd) if !proposals.exists(_ == cmd) =>
      log.info("request message received in replica actor")
      if (slot_out + Replica.WINDOW > slot_in) {
        clientInfo = clientInfo.updated(cmd.clientId, sender())
        requests = cmd :: requests
      } else {
        log.info("message is ignored")
      }

    /**
      * update decisions queue, but do not run RSM because decisions might not full
      * 1. if command is reconfiguration, do not execute command
      * 2. if command already been executed, skip command execution
      */
    case DecisionMessage(slotNumber, cmd) if slotNumber >= slot_out =>
      log.info("decision message received in replica actor")

      decisions = decisions.updated(slotNumber, cmd)

      while (decisions.isDefinedAt(slot_out)) {

        proposals.find(msg => msg.slotNumber == slot_out) match {
          case Some(msg) if msg.cmd == cmd => proposals = proposals.filterNot(_.slotNumber == slot_out)
          case Some(msg) => requests = msg.cmd :: requests
          case None =>

        }

        val toBeExecutedCmd = decisions.apply(slot_out)

        if (!toBeExecutedCmd.op.isInstanceOf[Config] && !decisions.exists { case (k, v) => k < slot_out && v == toBeExecutedCmd }) {
          log.info(s"replica execute $toBeExecutedCmd at index $slot_out")
          execute(toBeExecutedCmd)
          slot_out += 1
          clientInfo.get(toBeExecutedCmd.clientId).foreach(_ ! ResponseMessage(toBeExecutedCmd.clientId))

        } else {
          slot_out += 1
        }
      }


    case LookUpReplicaState =>
      this.internalState()
      sender() ! ReplicaState(slot_in, slot_out, clientInfo, requests, proposals, decisions, this.internalState())

    case msg =>
      log.warning(s"replica actor receive unknown message: $msg")

  }


  /**
    * slot_in should always > slot_out, so check those two value before sending proposal
    */
  def propose = {
    log.info("replica execute propose method")
    requests.foreach(cmd => {
      slot_in = Math.max(slot_in, slot_out)

      val proposal = ProposalMessage(slot_in, cmd)

      leaders.foreach(leader => leader ! proposal)

      proposals = proposal :: proposals

      slot_in += 1
    })
    requests = Nil

  }
}
```


Replica Actor 还是比较简单的，它本身不需要维系很多状态

### Acceptor

Acceptor 和 Replica 类似，本身也不需要维系很多状态，所有逻辑都在一个 actor 里面

伪代码

```
process Acceptor()
   var ballot_num := null, accepted = empty

   for ever
      switch received()
         case (p1a, seq, b)
            if b > ballot_num then
               ballot_num := b
            end if 
            send (seq, <p1b, self, ballot_num, accepted)

         case (p2a, seq, (b, s, c))
            if b == ballot_num then
               accepted := accepted + (b, s,c)
            end if 
            send(seq, <p2b, self, ballot_num) // 成功
```

Restrictions:

1: An acceptor a can only accept (b, s, c) if b == a.ballot_num

2: Acceptor a cannot remove pvalues from a.accepted

3: suppose that for each acceptor among a majority of acceptors, (b, s, c) belong to a.accepted. If b' > b and (b', s, c')
belong to a`.accepted, then c = c'


第三点在一定程度上解释了为什么 p2a 收到后不修改 accepted, 但是解释的不够全面，因为我觉得修改是不会导致出错的。至于 c = c`, 他有一个前提是
他有 (b, s, c) 这个属性。 其实 acceptor 回复与否不影响正确性，因为他不属于 majority, 只有等到下一个 leader election 才会成为 majority
的可能性。



```scala
object Acceptor {
  implicit val timeout = Timeout(3 second)

  case object LookUpAcceptor
  case class AcceptorInternalState(maxBallotNumSeenSoFar: BallotNumber,
                                   acceptedNumber: Map[Int, PValue])

  def getAcceptorStatus(acceptor: ActorRef): AcceptorInternalState =
    Await.result(acceptor ? LookUpAcceptor, 3 second).asInstanceOf[AcceptorInternalState]
}
```

```scala
class Acceptor extends Actor with ActorLogging {

  var maxBallotNumSeenSoFar: BallotNumber = BallotNumber("", -1)

  /**
    *  key is slot, value is PValue which include ballotNumber as member
    */
  var acceptedValues = Map.empty[Int, PValue]

  override def receive: Receive = {
    /**
      * @todo what if ballotNum equals maxBallotNumber? should reply too, because resend phase1a
      */
    case Phase1a(ballotNumber) =>

      if (ballotNumber > maxBallotNumSeenSoFar) {
        log.info(s"update ballotNumber to $ballotNumber")
        maxBallotNumSeenSoFar = ballotNumber
      }

      sender() ! Phase1b(maxBallotNumSeenSoFar, acceptedValues)

    /**
      * why didn't respond when ballotNumber bigger than itself ?
      * 为什么 phase2 不会更新 ballotNumber 呢?
      * 这边肯定有 Bug, 假如一个 acceptor 脑列， 他再也不能与 leader 同步了，除非当前 leader 又发生了中断
      */
    case Phase2a(ballotNumber, slotNumber, cmd) =>
      if(ballotNumber == maxBallotNumSeenSoFar) {
        // 这个不确定, 如果两个 leader 发送了相同 slot number 但是值不一样, 这里该不该+更新 acceptedValue map 呢
        // 修改肯定是对 acceptor 来说是不对的, 但是即使这边发生了修改, 也不会影响 replica 的值
        acceptedValues = acceptedValues.updated(slotNumber, PValue(ballotNumber, slotNumber, cmd))
      }

      log.info("send phase2b to commander")

      // why send phase2b what ever
      sender() ! Phase2b(maxBallotNumSeenSoFar, slotNumber)

    case LookUpAcceptor =>
      sender() ! AcceptorInternalState(maxBallotNumSeenSoFar, acceptedValues)

    case _ =>
      log.debug("unknown message received in acceptor")
  }
}
```

### Commander

Commander 就是发出 p2a 的 actor

```
process Commander(leader, acceptors, replicas, (b,s,c))
   var waitfor = acceptors

   for all acceptors: send(acceptor, (b, s, c))

   for ever
      switch receive()
         case (p2b, a, b')
            if b = b' then
               waitfor = waitfor -a
               if waitfor < len(acceptor)/2
                  for all replicas send(replica, decision, s, c)
                  exit
               end if
            else
               send (leader, preempted, b')
               exit
            end if
      end switch
   end for
end process
```

Commander 和 Acceptor 的关系有些不紧密

```scala
class Commander(acceptors: Set[ActorRef],
                replicas: Set[ActorRef],
                slotNumber: Int,
                ballotNumber: BallotNumber,
                cmd: Command) extends Actor with ActorLogging {

  var waitForResponse = acceptors

  override def preStart(): Unit = acceptors.foreach(_ ! Phase2a(ballotNumber, slotNumber, cmd))

  override def receive: Receive = {
    case Phase2b(_ballotNumber: BallotNumber, slotNumber: Int) =>
      val acceptor = sender()

      if (_ballotNumber > ballotNumber) {
        log.info("commander detected bigger ballotNumber, send preempted message to leader and" +
          "start another leader election")
        context.parent ! PreemptedMessage(_ballotNumber)
        context.stop(self)
      } else if (_ballotNumber == ballotNumber && waitForResponse.contains(acceptor)) {
        waitForResponse -= acceptor
        if (waitForResponse.size < (replicas.size + 1) / 2) {
          replicas.foreach(_ ! DecisionMessage( slotNumber, cmd))
//          context.parent ! DecisionMessage(slotNumber, cmd)
          context.stop(self)
        }
      }
  }
}
```

### Scout

```scala
process Scout(leader, acceptors, b)
   var wairFor = acceptors, pvalues = empty queue

   for for acceptors, send(acceptor, (p1a, self, b))

   for ever
      switch receive()
         case (p1b, acceptor, b`, p)
            if b` == b then
               pvalues = pvalue + p
               waitFor -= acceptor
               if |waitFor| < |acceptors|/2 then
                  send(leader, adopted, b, pvalues)
                  exit
               end if
            else
               send(leader, preempted)
            exit
         end case
      send switch
   end for
end process
```

当 b` != b 时，肯定是 preempted, 这是因为 Acceptor 返回的是他看到的最大值，并不一定非得这么写，主要看他与 receive 的关系

```scala
class Scout(acceptors: Set[ActorRef], ballotNumber: BallotNumber) extends Actor with ActorLogging {

  override def preStart(): Unit = {
    acceptors.foreach(_ ! Phase1a(ballotNumber))
  }

  var waitingAcceptor = acceptors
  var acceptedValues = Set.empty[PValue]

  var maxBallotNumberSeenSoFar = BallotNumber("", -1)

  override def receive: Receive = {
    case Phase1b(_ballotNumber, accepted) =>

      if (_ballotNumber > ballotNumber) {
        log.info("scout detect bigger ballotNumber, start another leader election")
        context.parent ! PreemptedMessage(ballotNumber)
        context.stop(self)
      } else {
        val acceptor = sender()

        // recognized by client
        if (_ballotNumber == ballotNumber && waitingAcceptor.contains(acceptor)) {
          waitingAcceptor = waitingAcceptor - acceptor
          acceptedValues ++= accepted.values.toSet
        }

        if (waitingAcceptor.size < (acceptors.size + 1) / 2) {
          log.info("collected enough message, send adopted message to leader")
          context.parent ! AdoptedMessage(ballotNumber, acceptedValues)
          context.stop(self)
        }
      }
  }
}
```

### Leader 

Leader 是最复杂的角色，在经典 paxos 算法中，他的名字叫 proposer

```
process Leader(acceptors, replicas)
   var ballot_num = (self, 0), active = false, proposal = empty queue
   spawn(Scout(self, acceptor, ballot_num))

   for ever
      switch receive()
         case (Proposal, slot, cmd)
            if NOT EXIST (s, c`) belong to proposals
               proposals = proposal + (slot, cmd)
               if active then
                  spawn(Commander(self, acceptor, replicas, (ballot_num, s, c)))
               end if
            end if
         end case

         case (adopted, ballot_num, pvals)
            proposals = proposal <- pmax(pvals)
            FOR ALL (s, c) belong to proposals
               spawn(Commander(self, replicas, (ballot_num, s, c)))
               active = true
         end case
         case (preempted, r`, leader`)
            if (r`, leader`) > ballot_num then
               active = false
               ballot_num = (r`+1, self)
               spawn(Scout(self, acceptors, ballot_num))
            end if
         end case
      end switch
   end for
end process
```

这里面有两个需要注意的点，第一点是 pmax 第二个是 <-, 好在他们都在同一行，也比较容易理解。当 leader 收到 preempted 时，他会自动把自己的 ballot_num +1 然后继续竞争，这会导致活锁

```scala
  def pmax(pvals: Set[PValue]): Map[Int, PValue] = {
    pvals.groupBy(_.slot_number)
      .map { case (slotNum, pvalSet) => (slotNum, pvalSet.maxBy(_.ballotNumber)) }
  }

  /**
    * x ▹ y returns everything in y and (x-y)
    */
  def ▹(proposals: Map[Int, ProposalMessage], map: Map[Int, PValue]): Map[Int, ProposalMessage] = {
    var combined = proposals
    map.foreach { case (slot, pv) => combined = combined.updated(slot,
      ProposalMessage(slot, pv.cmd))
    }
    combined
  }
```
符号的意思是用 acceptor 的符号来替换用户的请求

```scala
class Leader(var acceptors: Set[ActorRef], var replicas: Set[ActorRef]) extends Actor with ActorLogging {

  var proposals: Map[Int, ProposalMessage] = Map.empty[Int, ProposalMessage]
  var currentBallotNumber = BallotNumber(self.path.name, 1)

  @volatile
  var isActive = false

  def setActive(result: Boolean): Unit = {
    log.debug(s"set isActive to $result")
    isActive = result
  }

  override def preStart() = {
    context.actorOf(Scout.props(acceptors, currentBallotNumber), "scout-actor-for-leader" + Random.nextLong())
  }

  override def receive: Receive = inActive orElse shared

  def inActive: Receive = {

    // 能不能对 cmd 进行重复过滤?
    // 假如过滤的话, slotNumber 就会被浪费了, 导致 rsm 中间有了个缝
    // 但不确定是否完全正确
    case msg@ProposalMessage(slotNumber, cmd) =>

      log.info(s"leader receive proposal message from replica $msg in inactive mode")
      proposals.get(slotNumber) match {
        case None => proposals = proposals.updated(slotNumber, msg)
        case _ =>
      }

    case AdoptedMessage(ballotNumber, accepted) =>

      setActive(true)
      log.info(s"win leader election with ballotNum $ballotNumber, accepted $accepted")

      val slotValue = pmax(accepted)
      proposals = ▹(proposals, slotValue)
      // release commander actor
      proposals.foreach { case (slotNum: Int, proposal: ProposalMessage) =>
        context.actorOf(Commander.props(acceptors, replicas, slotNum, currentBallotNumber, proposal.cmd),
          name = "commander-actor-" + Random.nextLong())
      }

      context.become(active orElse shared)
  }

  def active: Receive = {

    case msg@ProposalMessage(slotNumber, cmd) =>
      log.info(s"leader receive proposal message in active mode slot $slotNumber, cmd: $cmd")
      proposals.get(slotNumber) match {
        case None =>
          proposals = proposals.updated(slotNumber, msg)
          // release commander actor
          context.actorOf(Commander.props(acceptors, replicas, slotNumber, currentBallotNumber, cmd),
            "commander-actor-" + Random.nextLong())
        case _ =>
      }
  }

  def shared: Receive = {
    case PreemptedMessage(ballotNumber: BallotNumber) =>
      log.info("preemptedMessage received in leader")
      if (ballotNumber > currentBallotNumber)
        currentBallotNumber = currentBallotNumber.copy(round = ballotNumber.round + 1)

      // another round of leader election
      // @todo backoff
      setActive(false)
      context.actorOf(Scout.props(acceptors, currentBallotNumber), "scout-actor-for-leader" + +Random.nextLong())
      context.become(inActive orElse shared)

    case LookUpStatus =>
      log.info("look up acceptor status")
      sender() ! LeaderInternalState(proposals, currentBallotNumber, isActive)

    case _ =>
  }
}
```







### Resource

[understanding paxos](https://understandingpaxos.wordpress.com/)


